{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# é™„å½• 10.3: æœç´¢ä¸æ£€ç´¢ (Search & Retrieval)\n",
    "\n",
    "## æ¦‚è¿°\n",
    "æ‚¨çŸ¥é“å¯ä»¥ä½¿ç”¨ Claude æ¥**ä¸ºæ‚¨æœç´¢ç»´åŸºç™¾ç§‘**å—ï¼ŸClaude å¯ä»¥æŸ¥æ‰¾å’Œæ£€ç´¢æ–‡ç« ï¼Œç„¶åæ‚¨è¿˜å¯ä»¥ä½¿ç”¨ Claude æ¥æ€»ç»“å’Œç»¼åˆè¿™äº›æ–‡ç« ï¼Œä»æ‰¾åˆ°çš„å†…å®¹ä¸­ç¼–å†™æ–°é¢–çš„å†…å®¹ï¼Œç­‰ç­‰ã€‚ä¸ä»…ä»…æ˜¯ç»´åŸºç™¾ç§‘ï¼æ‚¨è¿˜å¯ä»¥æœç´¢è‡ªå·±çš„æ–‡æ¡£ï¼Œæ— è®ºæ˜¯å­˜å‚¨ä¸ºçº¯æ–‡æœ¬è¿˜æ˜¯åµŒå…¥åœ¨å‘é‡æ•°æ®åº“ä¸­ã€‚\n",
    "\n",
    "## ä»€ä¹ˆæ˜¯ RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)?\n",
    "RAG (Retrieval-Augmented Generation) æ˜¯ä¸€ç§å°†å¤–éƒ¨çŸ¥è¯†æ£€ç´¢ä¸å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆèƒ½åŠ›ç›¸ç»“åˆçš„æŠ€æœ¯ï¼š\n",
    "\n",
    "### RAG çš„æ ¸å¿ƒç»„ä»¶ï¼š\n",
    "1. **æ£€ç´¢å™¨ (Retriever)**: ä»çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯\n",
    "2. **ç”Ÿæˆå™¨ (Generator)**: åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯ç”Ÿæˆå›ç­”\n",
    "3. **çŸ¥è¯†åº“ (Knowledge Base)**: å­˜å‚¨å¾…æ£€ç´¢çš„æ–‡æ¡£å’Œæ•°æ®\n",
    "\n",
    "### RAG çš„ä¼˜åŠ¿ï¼š\n",
    "- **å®æ—¶æ€§**: èƒ½å¤Ÿè·å–æœ€æ–°ä¿¡æ¯ï¼Œä¸å—æ¨¡å‹è®­ç»ƒæ—¶é—´é™åˆ¶\n",
    "- **å‡†ç¡®æ€§**: åŸºäºçœŸå®æ•°æ®æºï¼Œå‡å°‘å¹»è§‰ç°è±¡\n",
    "- **å¯æ§æ€§**: å¯ä»¥æŒ‡å®šç‰¹å®šçš„çŸ¥è¯†æº\n",
    "- **å¯æ‰©å±•æ€§**: éšæ—¶æ›´æ–°çŸ¥è¯†åº“å†…å®¹\n",
    "\n",
    "## å­¦ä¹ èµ„æº\n",
    "\n",
    "æŸ¥çœ‹æˆ‘ä»¬çš„ [RAG å®è·µæ•™ç¨‹ç¤ºä¾‹](https://github.com/anthropics/anthropic-cookbook/blob/main/third_party/Wikipedia/wikipedia-search-cookbook.ipynb) æ¥å­¦ä¹ å¦‚ä½•é€šè¿‡ä»å‘é‡æ•°æ®åº“ã€ç»´åŸºç™¾ç§‘ã€äº’è”ç½‘ç­‰æ£€ç´¢çš„æ•°æ®æ¥è¡¥å…… Claude çš„çŸ¥è¯†ï¼Œæé«˜ Claude å“åº”çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚åœ¨é‚£é‡Œï¼Œæ‚¨è¿˜å¯ä»¥å­¦ä¹ å¦‚ä½•ä½¿ç”¨æŸäº› [åµŒå…¥æŠ€æœ¯ (embeddings)](https://docs.anthropic.com/claude/docs/embeddings) å’Œå‘é‡æ•°æ®åº“å·¥å…·ã€‚\n",
    "\n",
    "å¦‚æœæ‚¨æœ‰å…´è¶£äº†è§£ä½¿ç”¨ Claude çš„é«˜çº§ RAG æ¶æ„ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„ [Claude 3 RAG æ¶æ„æŠ€æœ¯æ¼”ç¤ºå¹»ç¯ç‰‡](https://docs.google.com/presentation/d/1zxkSI7lLUBrZycA-_znwqu8DDyVhHLkQGScvzaZrUns/edit#slide=id.g2c736259dac_63_782)ã€‚\n",
    "\n",
    "## æŠ€æœ¯æ¶æ„å›¾\n",
    "```\n",
    "ç”¨æˆ·æŸ¥è¯¢ â†’ å‘é‡æ£€ç´¢ â†’ ç›¸å…³æ–‡æ¡£ â†’ Claudeå¤„ç† â†’ ç”Ÿæˆå›ç­”\n",
    "    â†“           â†“          â†“         â†“         â†“\n",
    "  Query    Embeddings   Context   Generate  Response\n",
    "```\n",
    "\n",
    "## åç»­ç« èŠ‚é¢„å‘Š\n",
    "åœ¨æ¥ä¸‹æ¥çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†æ¼”ç¤ºï¼š\n",
    "- å¦‚ä½•è®¾ç½®å‘é‡æ•°æ®åº“\n",
    "- å®ç°æ–‡æ¡£åµŒå…¥å’Œæ£€ç´¢\n",
    "- ä¸ Claude API é›†æˆ\n",
    "- æ„å»ºå®Œæ•´çš„ RAG åº”ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯å¢ƒè®¾ç½®å’Œä¾èµ–å®‰è£…\n",
    "# é¦–å…ˆå®‰è£…å¿…è¦çš„ä¾èµ–åŒ…\n",
    "\n",
    "# !pip install anthropic wikipedia-api sentence-transformers faiss-cpu numpy\n",
    "\n",
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import anthropic\n",
    "import wikipedia\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "\n",
    "# è®¾ç½® Anthropic API å¯†é’¥\n",
    "# è¯·ç¡®ä¿åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½® ANTHROPIC_API_KEY\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\")  # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥\n",
    ")\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒè®¾ç½®å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. åŸºç¡€ç»´åŸºç™¾ç§‘æœç´¢ç¤ºä¾‹\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬æ¥å®ç°ä¸€ä¸ªç®€å•çš„ç»´åŸºç™¾ç§‘æœç´¢åŠŸèƒ½ï¼Œè®© Claude èƒ½å¤Ÿè·å–å¹¶å¤„ç†ç»´åŸºç™¾ç§‘çš„å†…å®¹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaSearcher:\n",
    "    \"\"\"\n",
    "    ç»´åŸºç™¾ç§‘æœç´¢å™¨ç±»\n",
    "    ç”¨äºæœç´¢ã€è·å–å’Œå¤„ç†ç»´åŸºç™¾ç§‘æ–‡ç« å†…å®¹\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, language='zh'):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–ç»´åŸºç™¾ç§‘æœç´¢å™¨\n",
    "        \n",
    "        Args:\n",
    "            language (str): ç»´åŸºç™¾ç§‘è¯­è¨€ç‰ˆæœ¬ï¼Œé»˜è®¤ä¸ºä¸­æ–‡ 'zh'\n",
    "        \"\"\"\n",
    "        wikipedia.set_lang(language)\n",
    "        self.language = language\n",
    "    \n",
    "    def search_articles(self, query: str, max_results: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        æœç´¢ç»´åŸºç™¾ç§‘æ–‡ç« æ ‡é¢˜\n",
    "        \n",
    "        Args:\n",
    "            query (str): æœç´¢å…³é”®è¯\n",
    "            max_results (int): æœ€å¤§è¿”å›ç»“æœæ•°é‡\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: æ–‡ç« æ ‡é¢˜åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # æœç´¢ç›¸å…³æ–‡ç« æ ‡é¢˜\n",
    "            titles = wikipedia.search(query, results=max_results)\n",
    "            print(f\"ğŸ” æ‰¾åˆ° {len(titles)} ç¯‡ç›¸å…³æ–‡ç« :\")\n",
    "            for i, title in enumerate(titles, 1):\n",
    "                print(f\"  {i}. {title}\")\n",
    "            return titles\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æœç´¢å¤±è´¥: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_article_content(self, title: str, max_sentences: int = 10) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        è·å–ç»´åŸºç™¾ç§‘æ–‡ç« å†…å®¹\n",
    "        \n",
    "        Args:\n",
    "            title (str): æ–‡ç« æ ‡é¢˜\n",
    "            max_sentences (int): æœ€å¤§è¿”å›å¥å­æ•°é‡\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: åŒ…å«æ ‡é¢˜ã€æ‘˜è¦ã€å†…å®¹ç­‰ä¿¡æ¯çš„å­—å…¸\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # è·å–æ–‡ç« é¡µé¢\n",
    "            page = wikipedia.page(title)\n",
    "            \n",
    "            # å°†å†…å®¹åˆ†å‰²ä¸ºå¥å­å¹¶é™åˆ¶æ•°é‡\n",
    "            sentences = page.content.split('. ')[:max_sentences]\n",
    "            limited_content = '. '.join(sentences)\n",
    "            \n",
    "            article_info = {\n",
    "                'title': page.title,\n",
    "                'url': page.url,\n",
    "                'summary': page.summary,\n",
    "                'content': limited_content,\n",
    "                'length': len(page.content)\n",
    "            }\n",
    "            \n",
    "            print(f\"ğŸ“„ æˆåŠŸè·å–æ–‡ç« : {page.title}\")\n",
    "            print(f\"ğŸ“Š æ–‡ç« é•¿åº¦: {len(page.content)} å­—ç¬¦\")\n",
    "            \n",
    "            return article_info\n",
    "            \n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            print(f\"âš ï¸  å­˜åœ¨æ­§ä¹‰ï¼Œå¯é€‰æ‹©: {e.options[:5]}\")\n",
    "            # é€‰æ‹©ç¬¬ä¸€ä¸ªé€‰é¡¹\n",
    "            return self.get_article_content(e.options[0], max_sentences)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è·å–æ–‡ç« å¤±è´¥: {e}\")\n",
    "            return {}\n",
    "\n",
    "# åˆ›å»ºç»´åŸºç™¾ç§‘æœç´¢å™¨å®ä¾‹\n",
    "wiki_searcher = WikipediaSearcher()\n",
    "\n",
    "# ç¤ºä¾‹ï¼šæœç´¢å…³äºäººå·¥æ™ºèƒ½çš„æ–‡ç« \n",
    "search_query = \"äººå·¥æ™ºèƒ½\"\n",
    "articles = wiki_searcher.search_articles(search_query, max_results=3)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. é›†æˆ Claude è¿›è¡Œå†…å®¹åˆ†æ\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬å°†ç»´åŸºç™¾ç§‘æœç´¢ç»“æœä¸ Claude ç»“åˆï¼Œè®© Claude åˆ†æå’Œæ€»ç»“æ£€ç´¢åˆ°çš„å†…å®¹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaudeRAGProcessor:\n",
    "    \"\"\"\n",
    "    Claude RAG å¤„ç†å™¨\n",
    "    ç»“åˆç»´åŸºç™¾ç§‘æœç´¢å’Œ Claude åˆ†æèƒ½åŠ›\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, wiki_searcher: WikipediaSearcher):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ– Claude RAG å¤„ç†å™¨\n",
    "        \n",
    "        Args:\n",
    "            wiki_searcher (WikipediaSearcher): ç»´åŸºç™¾ç§‘æœç´¢å™¨å®ä¾‹\n",
    "        \"\"\"\n",
    "        self.wiki_searcher = wiki_searcher\n",
    "        \n",
    "    def search_and_analyze(self, user_query: str, max_articles: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        æœç´¢ç›¸å…³æ–‡ç« å¹¶ä½¿ç”¨ Claude è¿›è¡Œåˆ†æ\n",
    "        \n",
    "        Args:\n",
    "            user_query (str): ç”¨æˆ·æŸ¥è¯¢\n",
    "            max_articles (int): æœ€å¤§æœç´¢æ–‡ç« æ•°é‡\n",
    "            \n",
    "        Returns:\n",
    "            str: Claude åˆ†æåçš„å“åº”\n",
    "        \"\"\"\n",
    "        print(f\"ğŸš€ å¼€å§‹å¤„ç†æŸ¥è¯¢: {user_query}\")\n",
    "        \n",
    "        # 1. æœç´¢ç›¸å…³æ–‡ç« \n",
    "        article_titles = self.wiki_searcher.search_articles(user_query, max_articles)\n",
    "        \n",
    "        if not article_titles:\n",
    "            return \"âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡ç« \"\n",
    "        \n",
    "        # 2. è·å–æ–‡ç« å†…å®¹\n",
    "        articles_content = []\n",
    "        for title in article_titles[:max_articles]:\n",
    "            article = self.wiki_searcher.get_article_content(title, max_sentences=8)\n",
    "            if article:\n",
    "                articles_content.append(article)\n",
    "        \n",
    "        # 3. æ„å»º Claude æç¤ºè¯\n",
    "        context = self._build_context(articles_content)\n",
    "        prompt = self._build_prompt(user_query, context)\n",
    "        \n",
    "        # 4. è°ƒç”¨ Claude API\n",
    "        try:\n",
    "            response = client.messages.create(\n",
    "                model=\"claude-3-sonnet-20240229\",  # ä½¿ç”¨ Claude 3 Sonnet æ¨¡å‹\n",
    "                max_tokens=1500,\n",
    "                temperature=0.3,  # è¾ƒä½çš„æ¸©åº¦ä»¥ç¡®ä¿å‡†ç¡®æ€§\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            return response.content[0].text\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"âŒ Claude API è°ƒç”¨å¤±è´¥: {e}\"\n",
    "    \n",
    "    def _build_context(self, articles: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"\n",
    "        æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯\n",
    "        \n",
    "        Args:\n",
    "            articles (List[Dict]): æ–‡ç« åˆ—è¡¨\n",
    "            \n",
    "        Returns:\n",
    "            str: æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²\n",
    "        \"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        for i, article in enumerate(articles, 1):\n",
    "            context_part = f\"\"\"\n",
    "ã€æ–‡ç«  {i}ã€‘\n",
    "æ ‡é¢˜: {article['title']}\n",
    "é“¾æ¥: {article['url']}\n",
    "æ‘˜è¦: {article['summary']}\n",
    "å†…å®¹èŠ‚é€‰: {article['content'][:1000]}...\n",
    "\"\"\"\n",
    "            context_parts.append(context_part)\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def _build_prompt(self, user_query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        æ„å»º Claude æç¤ºè¯\n",
    "        \n",
    "        Args:\n",
    "            user_query (str): ç”¨æˆ·æŸ¥è¯¢\n",
    "            context (str): ä¸Šä¸‹æ–‡ä¿¡æ¯\n",
    "            \n",
    "        Returns:\n",
    "            str: å®Œæ•´çš„æç¤ºè¯\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åŠ©æ‰‹ã€‚åŸºäºä»¥ä¸‹ä»ç»´åŸºç™¾ç§‘æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œè¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n",
    "\n",
    "ç”¨æˆ·é—®é¢˜ï¼š{user_query}\n",
    "\n",
    "å‚è€ƒèµ„æ–™ï¼š\n",
    "{context}\n",
    "\n",
    "è¯·æ ¹æ®ä¸Šè¿°èµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œè¦æ±‚ï¼š\n",
    "1. å›ç­”è¦å‡†ç¡®ã€å…¨é¢\n",
    "2. å¦‚æœèµ„æ–™ä¸­æœ‰å…·ä½“æ•°æ®æˆ–äº‹å®ï¼Œè¯·å¼•ç”¨\n",
    "3. ä¿æŒå®¢è§‚å’Œä¸­ç«‹çš„è¯­è°ƒ\n",
    "4. å¦‚æœèµ„æ–™ä¸è¶³ä»¥å®Œå…¨å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜\n",
    "5. åœ¨å›ç­”æœ«å°¾æä¾›å‚è€ƒæ–‡ç« çš„é“¾æ¥\n",
    "\n",
    "å›ç­”ï¼š\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "# åˆ›å»º Claude RAG å¤„ç†å™¨\n",
    "rag_processor = ClaudeRAGProcessor(wiki_searcher)\n",
    "\n",
    "# ç¤ºä¾‹æŸ¥è¯¢\n",
    "user_question = \"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿå®ƒæœ‰å“ªäº›ä¸»è¦åº”ç”¨é¢†åŸŸï¼Ÿ\"\n",
    "print(f\"ç”¨æˆ·é—®é¢˜: {user_question}\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰§è¡Œ RAG æŸ¥è¯¢ç¤ºä¾‹\n",
    "# æ³¨æ„ï¼šéœ€è¦è®¾ç½® ANTHROPIC_API_KEY ç¯å¢ƒå˜é‡æ‰èƒ½è¿è¡Œ\n",
    "\n",
    "# å–æ¶ˆæ³¨é‡Šä¸‹é¢çš„ä»£ç æ¥è¿è¡Œç¤ºä¾‹\n",
    "# result = rag_processor.search_and_analyze(user_question)\n",
    "# print(\"Claude çš„å›ç­”ï¼š\")\n",
    "# print(result)\n",
    "\n",
    "# æ¨¡æ‹Ÿè¾“å‡ºç¤ºä¾‹\n",
    "print(\"Claude çš„å›ç­”ï¼š\")\n",
    "print(\"\"\"\n",
    "åŸºäºç»´åŸºç™¾ç§‘çš„èµ„æ–™ï¼Œæˆ‘æ¥å›ç­”æ‚¨å…³äºæœºå™¨å­¦ä¹ çš„é—®é¢˜ï¼š\n",
    "\n",
    "**ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ**\n",
    "\n",
    "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒæ˜¯ä¸€ç§è®©è®¡ç®—æœºåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ çš„æ–¹æ³•ã€‚æœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡åˆ†ææ•°æ®æ¥è¯†åˆ«æ¨¡å¼ï¼Œå¹¶åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚\n",
    "\n",
    "**ä¸»è¦åº”ç”¨é¢†åŸŸï¼š**\n",
    "\n",
    "1. **å›¾åƒè¯†åˆ«**: äººè„¸è¯†åˆ«ã€åŒ»å­¦å½±åƒåˆ†æã€è‡ªåŠ¨é©¾é©¶æ±½è½¦çš„è§†è§‰ç³»ç»Ÿ\n",
    "2. **è‡ªç„¶è¯­è¨€å¤„ç†**: æœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ã€èŠå¤©æœºå™¨äºº\n",
    "3. **æ¨èç³»ç»Ÿ**: ç”µå•†å¹³å°ã€æµåª’ä½“æœåŠ¡çš„ä¸ªæ€§åŒ–æ¨è\n",
    "4. **é‡‘èæœåŠ¡**: é£é™©è¯„ä¼°ã€æ¬ºè¯ˆæ£€æµ‹ã€ç®—æ³•äº¤æ˜“\n",
    "5. **åŒ»ç–—å¥åº·**: ç–¾ç—…è¯Šæ–­ã€è¯ç‰©å‘ç°ã€åŸºå› åˆ†æ\n",
    "6. **äº¤é€šè¿è¾“**: è‡ªåŠ¨é©¾é©¶ã€äº¤é€šä¼˜åŒ–ã€è·¯çº¿è§„åˆ’\n",
    "\n",
    "å‚è€ƒèµ„æ–™ï¼š\n",
    "- æœºå™¨å­¦ä¹  - https://zh.wikipedia.org/wiki/æœºå™¨å­¦ä¹ \n",
    "- äººå·¥æ™ºèƒ½ - https://zh.wikipedia.org/wiki/äººå·¥æ™ºèƒ½\n",
    "- æ·±åº¦å­¦ä¹  - https://zh.wikipedia.org/wiki/æ·±åº¦å­¦ä¹ \n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. é«˜çº§å‘é‡æœç´¢ä¸è¯­ä¹‰æ£€ç´¢\n",
    "\n",
    "é™¤äº†ç®€å•çš„å…³é”®è¯æœç´¢ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨å‘é‡åµŒå…¥æŠ€æœ¯å®ç°æ›´æ™ºèƒ½çš„è¯­ä¹‰æœç´¢ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿç†è§£æŸ¥è¯¢çš„å«ä¹‰ï¼Œè€Œä¸ä»…ä»…æ˜¯åŒ¹é…å…³é”®è¯ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorSearchEngine:\n",
    "    \"\"\"\n",
    "    å‘é‡æœç´¢å¼•æ“\n",
    "    ä½¿ç”¨è¯­ä¹‰åµŒå…¥è¿›è¡Œæ–‡æ¡£æ£€ç´¢\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å‘é‡æœç´¢å¼•æ“\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): å¥å­åµŒå…¥æ¨¡å‹åç§°\n",
    "        \"\"\"\n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "        self.documents = []  # å­˜å‚¨æ–‡æ¡£\n",
    "        self.embeddings = None  # å­˜å‚¨æ–‡æ¡£åµŒå…¥\n",
    "        self.index = None  # FAISS ç´¢å¼•\n",
    "        \n",
    "        print(f\"âœ… å‘é‡æœç´¢å¼•æ“åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {model_name}\")\n",
    "    \n",
    "    def add_documents(self, documents: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        æ·»åŠ æ–‡æ¡£åˆ°æœç´¢å¼•æ“\n",
    "        \n",
    "        Args:\n",
    "            documents (List[Dict]): æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å« 'title', 'content' ç­‰å­—æ®µ\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ“š æ­£åœ¨æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£...\")\n",
    "        \n",
    "        # å­˜å‚¨æ–‡æ¡£\n",
    "        self.documents.extend(documents)\n",
    "        \n",
    "        # å‡†å¤‡ç”¨äºåµŒå…¥çš„æ–‡æœ¬\n",
    "        texts_for_embedding = []\n",
    "        for doc in documents:\n",
    "            # ç»„åˆæ ‡é¢˜å’Œå†…å®¹ç”¨äºåµŒå…¥\n",
    "            text = f\"{doc.get('title', '')} {doc.get('content', '')}\".strip()\n",
    "            texts_for_embedding.append(text)\n",
    "        \n",
    "        # ç”ŸæˆåµŒå…¥å‘é‡\n",
    "        print(\"ğŸ”„ æ­£åœ¨ç”Ÿæˆæ–‡æ¡£åµŒå…¥å‘é‡...\")\n",
    "        new_embeddings = self.encoder.encode(texts_for_embedding)\n",
    "        \n",
    "        # æ›´æ–°åµŒå…¥çŸ©é˜µ\n",
    "        if self.embeddings is None:\n",
    "            self.embeddings = new_embeddings\n",
    "        else:\n",
    "            self.embeddings = np.vstack([self.embeddings, new_embeddings])\n",
    "        \n",
    "        # é‡å»º FAISS ç´¢å¼•\n",
    "        self._build_index()\n",
    "        \n",
    "        print(f\"âœ… æ–‡æ¡£æ·»åŠ å®Œæˆï¼Œå½“å‰å…±æœ‰ {len(self.documents)} ä¸ªæ–‡æ¡£\")\n",
    "    \n",
    "    def _build_index(self):\n",
    "        \"\"\"æ„å»º FAISS ç´¢å¼•ç”¨äºå¿«é€Ÿç›¸ä¼¼åº¦æœç´¢\"\"\"\n",
    "        if self.embeddings is not None:\n",
    "            dimension = self.embeddings.shape[1]\n",
    "            self.index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦\n",
    "            \n",
    "            # å½’ä¸€åŒ–åµŒå…¥å‘é‡\n",
    "            faiss.normalize_L2(self.embeddings)\n",
    "            self.index.add(self.embeddings)\n",
    "            \n",
    "            print(f\"ğŸ” FAISS ç´¢å¼•æ„å»ºå®Œæˆï¼Œç»´åº¦: {dimension}\")\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        è¯­ä¹‰æœç´¢æ–‡æ¡£\n",
    "        \n",
    "        Args:\n",
    "            query (str): æŸ¥è¯¢æ–‡æœ¬\n",
    "            top_k (int): è¿”å›æœ€ç›¸ä¼¼çš„æ–‡æ¡£æ•°é‡\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: æœç´¢ç»“æœï¼ŒåŒ…å«æ–‡æ¡£å’Œç›¸ä¼¼åº¦åˆ†æ•°\n",
    "        \"\"\"\n",
    "        if self.index is None or len(self.documents) == 0:\n",
    "            print(\"âŒ æœç´¢å¼•æ“ä¸ºç©ºï¼Œè¯·å…ˆæ·»åŠ æ–‡æ¡£\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"ğŸ” æœç´¢æŸ¥è¯¢: '{query}'\")\n",
    "        \n",
    "        # å¯¹æŸ¥è¯¢è¿›è¡ŒåµŒå…¥\n",
    "        query_embedding = self.encoder.encode([query])\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # æ‰§è¡Œæœç´¢\n",
    "        scores, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        # æ•´ç†æœç´¢ç»“æœ\n",
    "        results = []\n",
    "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            if idx < len(self.documents):  # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ\n",
    "                result = {\n",
    "                    'rank': i + 1,\n",
    "                    'score': float(score),\n",
    "                    'document': self.documents[idx]\n",
    "                }\n",
    "                results.append(result)\n",
    "        \n",
    "        print(f\"ğŸ“Š æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£\")\n",
    "        return results\n",
    "    \n",
    "    def print_search_results(self, results: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        æ‰“å°æœç´¢ç»“æœ\n",
    "        \n",
    "        Args:\n",
    "            results (List[Dict]): æœç´¢ç»“æœ\n",
    "        \"\"\"\n",
    "        for result in results:\n",
    "            print(f\"\\næ’å {result['rank']} (ç›¸ä¼¼åº¦: {result['score']:.3f})\")\n",
    "            print(f\"æ ‡é¢˜: {result['document'].get('title', 'N/A')}\")\n",
    "            print(f\"å†…å®¹æ‘˜è¦: {result['document'].get('content', 'N/A')[:200]}...\")\n",
    "            if 'url' in result['document']:\n",
    "                print(f\"é“¾æ¥: {result['document']['url']}\")\n",
    "\n",
    "# åˆ›å»ºå‘é‡æœç´¢å¼•æ“å®ä¾‹\n",
    "vector_engine = VectorSearchEngine()\n",
    "\n",
    "# ç¤ºä¾‹ï¼šå‡†å¤‡ä¸€äº›æ–‡æ¡£æ•°æ®\n",
    "sample_documents = [\n",
    "    {\n",
    "        'title': 'æ·±åº¦å­¦ä¹ åŸºç¡€',\n",
    "        'content': 'æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤ºã€‚æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚',\n",
    "        'category': 'AIæŠ€æœ¯',\n",
    "        'url': 'https://example.com/deep-learning'\n",
    "    },\n",
    "    {\n",
    "        'title': 'è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯',\n",
    "        'content': 'è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œæ—¨åœ¨è®©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚åŒ…æ‹¬æ–‡æœ¬åˆ†æã€æœºå™¨ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æç­‰åº”ç”¨ã€‚',\n",
    "        'category': 'AIåº”ç”¨',\n",
    "        'url': 'https://example.com/nlp'\n",
    "    },\n",
    "    {\n",
    "        'title': 'è®¡ç®—æœºè§†è§‰åŸç†',\n",
    "        'content': 'è®¡ç®—æœºè§†è§‰æ˜¯è®©è®¡ç®—æœºè·å¾—ç±»ä¼¼äººç±»è§†è§‰ç³»ç»Ÿçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ç­‰æŠ€æœ¯ã€‚å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ç­‰é¢†åŸŸã€‚',\n",
    "        'category': 'AIåº”ç”¨',\n",
    "        'url': 'https://example.com/cv'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"å‡†å¤‡æ·»åŠ ç¤ºä¾‹æ–‡æ¡£åˆ°å‘é‡æœç´¢å¼•æ“...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¼”ç¤ºå‘é‡æœç´¢åŠŸèƒ½\n",
    "# æ³¨æ„ï¼šå®é™…è¿è¡Œéœ€è¦å®‰è£… sentence-transformers å’Œ faiss-cpu\n",
    "\n",
    "# å–æ¶ˆæ³¨é‡Šä¸‹é¢çš„ä»£ç æ¥è¿è¡Œå‘é‡æœç´¢ç¤ºä¾‹\n",
    "# vector_engine.add_documents(sample_documents)\n",
    "\n",
    "# æ‰§è¡Œä¸åŒç±»å‹çš„è¯­ä¹‰æœç´¢\n",
    "# search_queries = [\n",
    "#     \"å¦‚ä½•è®©æœºå™¨ç†è§£äººç±»è¯­è¨€ï¼Ÿ\",\n",
    "#     \"å›¾åƒè¯†åˆ«ç›¸å…³æŠ€æœ¯\",\n",
    "#     \"ç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹ \"\n",
    "# ]\n",
    "\n",
    "# for query in search_queries:\n",
    "#     print(f\"\\n{'='*50}\")\n",
    "#     results = vector_engine.search(query, top_k=2)\n",
    "#     vector_engine.print_search_results(results)\n",
    "\n",
    "# æ¨¡æ‹Ÿå‘é‡æœç´¢è¾“å‡º\n",
    "print(\"å‘é‡æœç´¢å¼•æ“æ¼”ç¤ºï¼š\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nğŸ” æœç´¢æŸ¥è¯¢: 'å¦‚ä½•è®©æœºå™¨ç†è§£äººç±»è¯­è¨€ï¼Ÿ'\")\n",
    "print(\"ğŸ“Š æ‰¾åˆ° 2 ä¸ªç›¸å…³æ–‡æ¡£\")\n",
    "print(\"\\næ’å 1 (ç›¸ä¼¼åº¦: 0.758)\")\n",
    "print(\"æ ‡é¢˜: è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯\")\n",
    "print(\"å†…å®¹æ‘˜è¦: è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œæ—¨åœ¨è®©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚åŒ…æ‹¬æ–‡æœ¬åˆ†æã€æœºå™¨ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æç­‰åº”ç”¨ã€‚\")\n",
    "print(\"é“¾æ¥: https://example.com/nlp\")\n",
    "\n",
    "print(\"\\næ’å 2 (ç›¸ä¼¼åº¦: 0.632)\")\n",
    "print(\"æ ‡é¢˜: æ·±åº¦å­¦ä¹ åŸºç¡€\")\n",
    "print(\"å†…å®¹æ‘˜è¦: æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤ºã€‚æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚\")\n",
    "print(\"é“¾æ¥: https://example.com/deep-learning\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nğŸ” æœç´¢æŸ¥è¯¢: 'å›¾åƒè¯†åˆ«ç›¸å…³æŠ€æœ¯'\")\n",
    "print(\"ğŸ“Š æ‰¾åˆ° 2 ä¸ªç›¸å…³æ–‡æ¡£\")\n",
    "print(\"\\næ’å 1 (ç›¸ä¼¼åº¦: 0.812)\")\n",
    "print(\"æ ‡é¢˜: è®¡ç®—æœºè§†è§‰åŸç†\")\n",
    "print(\"å†…å®¹æ‘˜è¦: è®¡ç®—æœºè§†è§‰æ˜¯è®©è®¡ç®—æœºè·å¾—ç±»ä¼¼äººç±»è§†è§‰ç³»ç»Ÿçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ç­‰æŠ€æœ¯ã€‚å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ç­‰é¢†åŸŸã€‚\")\n",
    "print(\"é“¾æ¥: https://example.com/cv\")\n",
    "\n",
    "print(\"\\næ’å 2 (ç›¸ä¼¼åº¦: 0.689)\")\n",
    "print(\"æ ‡é¢˜: æ·±åº¦å­¦ä¹ åŸºç¡€\")  \n",
    "print(\"å†…å®¹æ‘˜è¦: æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤ºã€‚æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚\")\n",
    "print(\"é“¾æ¥: https://example.com/deep-learning\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. RAG ç³»ç»Ÿè®¾è®¡æœ€ä½³å®è·µ\n",
    "\n",
    "### ğŸ—ï¸ ç³»ç»Ÿæ¶æ„è®¾è®¡åŸåˆ™\n",
    "\n",
    "#### 4.1 æ•°æ®é¢„å¤„ç†\n",
    "- **æ–‡æ¡£åˆ†å—**: å°†é•¿æ–‡æ¡£åˆ†å‰²æˆé€‚å½“å¤§å°çš„å—ï¼ˆé€šå¸¸ 200-500 tokensï¼‰\n",
    "- **å†…å®¹æ¸…æ´—**: å»é™¤æ— å…³æ ¼å¼ã€æ ‡ç­¾å’Œå™ªéŸ³\n",
    "- **å…ƒæ•°æ®æå–**: ä¿ç•™æ ‡é¢˜ã€æ—¥æœŸã€æ¥æºç­‰é‡è¦ä¿¡æ¯\n",
    "\n",
    "#### 4.2 åµŒå…¥ç­–ç•¥\n",
    "- **æ¨¡å‹é€‰æ‹©**: æ ¹æ®è¯­è¨€å’Œé¢†åŸŸé€‰æ‹©åˆé€‚çš„åµŒå…¥æ¨¡å‹\n",
    "- **æ‰¹é‡å¤„ç†**: æ‰¹é‡ç”ŸæˆåµŒå…¥ä»¥æé«˜æ•ˆç‡\n",
    "- **å¢é‡æ›´æ–°**: æ”¯æŒæ–°æ–‡æ¡£çš„å¢é‡æ·»åŠ \n",
    "\n",
    "#### 4.3 æ£€ç´¢ä¼˜åŒ–\n",
    "- **æ··åˆæœç´¢**: ç»“åˆå…³é”®è¯æœç´¢å’Œè¯­ä¹‰æœç´¢\n",
    "- **é‡æ’åº**: ä½¿ç”¨äº¤å‰ç¼–ç å™¨å¯¹æ£€ç´¢ç»“æœé‡æ–°æ’åº\n",
    "- **å¤šæ ·æ€§**: ç¡®ä¿æ£€ç´¢ç»“æœçš„å¤šæ ·æ€§ï¼Œé¿å…ä¿¡æ¯é‡å¤\n",
    "\n",
    "#### 4.4 ç”Ÿæˆç­–ç•¥\n",
    "- **ä¸Šä¸‹æ–‡ç®¡ç†**: æ§åˆ¶è¾“å…¥ç»™ LLM çš„ä¸Šä¸‹æ–‡é•¿åº¦\n",
    "- **å¼•ç”¨æœºåˆ¶**: åœ¨å›ç­”ä¸­åŒ…å«ä¿¡æ¯æ¥æº\n",
    "- **ä¸€è‡´æ€§æ£€æŸ¥**: éªŒè¯ç”Ÿæˆå†…å®¹ä¸æ£€ç´¢å†…å®¹çš„ä¸€è‡´æ€§\n",
    "\n",
    "### ğŸ“Š æ€§èƒ½è¯„ä¼°æŒ‡æ ‡\n",
    "\n",
    "1. **æ£€ç´¢è´¨é‡**\n",
    "   - Recall@K: å‰Kä¸ªç»“æœä¸­ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹\n",
    "   - Precision@K: å‰Kä¸ªç»“æœä¸­æ­£ç¡®ç­”æ¡ˆçš„æ¯”ä¾‹\n",
    "   - MRR (Mean Reciprocal Rank): å¹³å‡å€’æ•°æ’å\n",
    "\n",
    "2. **ç”Ÿæˆè´¨é‡**\n",
    "   - BLEU/ROUGE: ä¸å‚è€ƒç­”æ¡ˆçš„é‡å åº¦\n",
    "   - BERTScore: è¯­ä¹‰ç›¸ä¼¼åº¦è¯„åˆ†\n",
    "   - äººå·¥è¯„ä¼°: å‡†ç¡®æ€§ã€æµç•…æ€§ã€ç›¸å…³æ€§\n",
    "\n",
    "3. **ç³»ç»Ÿæ€§èƒ½**\n",
    "   - å“åº”æ—¶é—´: ç«¯åˆ°ç«¯æŸ¥è¯¢å¤„ç†æ—¶é—´\n",
    "   - ååé‡: æ¯ç§’å¤„ç†çš„æŸ¥è¯¢æ•°é‡\n",
    "   - èµ„æºä½¿ç”¨: CPUã€å†…å­˜ã€å­˜å‚¨å ç”¨\n",
    "\n",
    "### ğŸ”§ éƒ¨ç½²å»ºè®®\n",
    "\n",
    "#### æŠ€æœ¯æ ˆæ¨è\n",
    "```python\n",
    "# å‘é‡æ•°æ®åº“é€‰æ‹©\n",
    "# - FAISS: é€‚åˆåŸå‹å¼€å‘å’Œå°è§„æ¨¡éƒ¨ç½²\n",
    "# - Pinecone: æ‰˜ç®¡å‘é‡æ•°æ®åº“æœåŠ¡\n",
    "# - Weaviate: å¼€æºå‘é‡æ•°æ®åº“\n",
    "# - Qdrant: é«˜æ€§èƒ½å‘é‡æœç´¢å¼•æ“\n",
    "\n",
    "# åµŒå…¥æ¨¡å‹é€‰æ‹©\n",
    "# - ä¸­æ–‡: text2vec-chinese, uer/sbert-base-chinese-nli\n",
    "# - è‹±æ–‡: all-MiniLM-L6-v2, sentence-transformers/all-mpnet-base-v2\n",
    "# - å¤šè¯­è¨€: multilingual-e5-large\n",
    "\n",
    "# Webæ¡†æ¶\n",
    "# - FastAPI: é«˜æ€§èƒ½å¼‚æ­¥æ¡†æ¶\n",
    "# - Flask: è½»é‡çº§ç®€å•æ¡†æ¶\n",
    "# - Django: åŠŸèƒ½å®Œæ•´çš„Webæ¡†æ¶\n",
    "```\n",
    "\n",
    "#### ç”Ÿäº§ç¯å¢ƒè€ƒè™‘\n",
    "- **ç¼“å­˜ç­–ç•¥**: ç¼“å­˜å¸¸è§æŸ¥è¯¢ç»“æœ\n",
    "- **è´Ÿè½½å‡è¡¡**: å¤šå®ä¾‹éƒ¨ç½²ä»¥å¤„ç†é«˜å¹¶å‘\n",
    "- **ç›‘æ§å‘Šè­¦**: å®æ—¶ç›‘æ§ç³»ç»Ÿæ€§èƒ½å’Œé”™è¯¯ç‡\n",
    "- **å®‰å…¨é˜²æŠ¤**: API é™æµã€èº«ä»½éªŒè¯ã€æ•°æ®åŠ å¯†\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. æ€»ç»“ä¸è¿›é˜¶å­¦ä¹ \n",
    "\n",
    "### ğŸ¯ æœ¬ç« å…³é”®è¦ç‚¹\n",
    "\n",
    "1. **RAG æ ¸å¿ƒæ¦‚å¿µ**: æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„ä¼˜åŠ¿\n",
    "2. **å®ç°æ–¹æ³•**: ä»ç®€å•çš„ç»´åŸºç™¾ç§‘æœç´¢åˆ°å¤æ‚çš„å‘é‡è¯­ä¹‰æ£€ç´¢\n",
    "3. **æŠ€æœ¯æ ˆ**: Claude API + å‘é‡æ•°æ®åº“ + åµŒå…¥æ¨¡å‹çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ\n",
    "4. **æœ€ä½³å®è·µ**: æ•°æ®é¢„å¤„ç†ã€æ£€ç´¢ä¼˜åŒ–ã€ç”Ÿæˆç­–ç•¥çš„ç³»ç»Ÿè®¾è®¡\n",
    "\n",
    "### ğŸš€ è¿›é˜¶å­¦ä¹ è·¯å¾„\n",
    "\n",
    "#### åˆå­¦è€… (1-2å‘¨)\n",
    "- [ ] æŒæ¡ RAG åŸºæœ¬æ¦‚å¿µå’Œå·¥ä½œæµç¨‹\n",
    "- [ ] å®ç°ç®€å•çš„æ–‡æ¡£é—®ç­”ç³»ç»Ÿ\n",
    "- [ ] å­¦ä¹ ä½¿ç”¨ FAISS è¿›è¡Œå‘é‡æœç´¢\n",
    "- [ ] äº†è§£ä¸åŒçš„åµŒå…¥æ¨¡å‹ç‰¹ç‚¹\n",
    "\n",
    "#### ä¸­çº§å¼€å‘è€… (2-4å‘¨)\n",
    "- [ ] ä¼˜åŒ–æ£€ç´¢æ€§èƒ½å’Œç”Ÿæˆè´¨é‡\n",
    "- [ ] å®ç°æ··åˆæœç´¢ï¼ˆå…³é”®è¯+è¯­ä¹‰ï¼‰\n",
    "- [ ] å­¦ä¹ ä½¿ç”¨ä¸“ä¸šå‘é‡æ•°æ®åº“\n",
    "- [ ] è®¾è®¡è¯„ä¼°æŒ‡æ ‡å’Œæµ‹è¯•æ¡†æ¶\n",
    "\n",
    "#### é«˜çº§å·¥ç¨‹å¸ˆ (1-2æœˆ)\n",
    "- [ ] æ„å»ºç”Ÿäº§çº§ RAG ç³»ç»Ÿ\n",
    "- [ ] å®ç°å¤šæ¨¡æ€æ£€ç´¢ï¼ˆæ–‡æœ¬+å›¾åƒï¼‰\n",
    "- [ ] ä¼˜åŒ–å¤§è§„æ¨¡éƒ¨ç½²å’Œæ€§èƒ½\n",
    "- [ ] ç ”ç©¶å‰æ²¿ RAG æ¶æ„å’ŒæŠ€æœ¯\n",
    "\n",
    "### ğŸ“š æ¨èèµ„æº\n",
    "\n",
    "#### å®˜æ–¹æ–‡æ¡£\n",
    "- [Anthropic Claude API æ–‡æ¡£](https://docs.anthropic.com/claude/docs/intro-to-claude)\n",
    "- [LangChain RAG æ•™ç¨‹](https://python.langchain.com/docs/use_cases/question_answering)\n",
    "- [Sentence Transformers æ–‡æ¡£](https://www.sbert.net/)\n",
    "\n",
    "#### å¼€æºé¡¹ç›®\n",
    "- [RAG Cookbook](https://github.com/anthropics/anthropic-cookbook) - Anthropic å®˜æ–¹ç¤ºä¾‹\n",
    "- [LlamaIndex](https://github.com/run-llama/llama_index) - RAG æ¡†æ¶\n",
    "- [Haystack](https://github.com/deepset-ai/haystack) - NLP ç®¡é“å·¥å…·\n",
    "\n",
    "#### å­¦æœ¯è®ºæ–‡\n",
    "- \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" (Lewis et al., 2020)\n",
    "- \"Dense Passage Retrieval for Open-Domain Question Answering\" (Karpukhin et al., 2020)\n",
    "- \"FiD: Leveraging Passage Retrieval with Generative Models\" (Izacard & Grave, 2021)\n",
    "\n",
    "### ğŸ’¡ å®è·µå»ºè®®\n",
    "\n",
    "1. **ä»å°åšèµ·**: å…ˆåœ¨å°æ•°æ®é›†ä¸ŠéªŒè¯æ¦‚å¿µï¼Œå†æ‰©å±•åˆ°ç”Ÿäº§ç¯å¢ƒ\n",
    "2. **è¿­ä»£ä¼˜åŒ–**: æŒç»­æ”¶é›†ç”¨æˆ·åé¦ˆï¼Œä¼˜åŒ–æ£€ç´¢å’Œç”Ÿæˆæ•ˆæœ\n",
    "3. **ç›‘æ§æŒ‡æ ‡**: å»ºç«‹å®Œå–„çš„ç›‘æ§ä½“ç³»ï¼ŒåŠæ—¶å‘ç°å’Œè§£å†³é—®é¢˜\n",
    "4. **å®‰å…¨ç¬¬ä¸€**: æ³¨æ„æ•°æ®éšç§å’Œæ¨¡å‹å®‰å…¨ï¼Œé˜²æ­¢æ¶æ„æ”»å‡»\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ­å–œå®Œæˆæœç´¢ä¸æ£€ç´¢æŠ€æœ¯çš„å­¦ä¹ ï¼**\n",
    "\n",
    "ç°åœ¨æ‚¨å·²ç»æŒæ¡äº†ä½¿ç”¨ Claude æ„å»ºæ™ºèƒ½æœç´¢ç³»ç»Ÿçš„æ ¸å¿ƒæŠ€èƒ½ã€‚ç»§ç»­æ¢ç´¢å’Œå®è·µï¼Œå°†è¿™äº›æŠ€æœ¯åº”ç”¨åˆ°æ‚¨çš„é¡¹ç›®ä¸­å§ï¼\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

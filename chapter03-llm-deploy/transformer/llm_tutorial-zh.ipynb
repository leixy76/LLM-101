{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”§ ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒä¿¡æ¯æ£€æŸ¥\n",
        "\n",
        "åœ¨å¼€å§‹éƒ¨ç½²æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£å½“å‰çš„è¿è¡Œç¯å¢ƒã€‚è¿™ä¸ªæ­¥éª¤éå¸¸é‡è¦ï¼Œå› ä¸ºï¼š\n",
        "\n",
        "### ğŸ¯ æ£€æŸ¥ç›®çš„\n",
        "1. **ç¡¬ä»¶ç¡®è®¤**: ç¡®ä¿æœ‰è¶³å¤Ÿçš„ GPU æ˜¾å­˜è¿è¡Œæ¨¡å‹\n",
        "2. **ç³»ç»Ÿå…¼å®¹**: éªŒè¯æ“ä½œç³»ç»Ÿå’Œ Python ç‰ˆæœ¬\n",
        "3. **èµ„æºè¯„ä¼°**: äº†è§£å¯ç”¨çš„ CPUã€å†…å­˜å’Œå­˜å‚¨ç©ºé—´\n",
        "4. **ç¯å¢ƒé…ç½®**: æ£€æŸ¥ CUDA ç‰ˆæœ¬å’Œç›¸å…³ä¾èµ–\n",
        "\n",
        "### ğŸ“Š æ£€æŸ¥å†…å®¹\n",
        "- **æ“ä½œç³»ç»Ÿ**: Linux å‘è¡Œç‰ˆå’Œç‰ˆæœ¬\n",
        "- **CPU ä¿¡æ¯**: å¤„ç†å™¨å‹å·å’Œæ ¸å¿ƒæ•°\n",
        "- **å†…å­˜çŠ¶æ€**: æ€»å†…å­˜å’Œå¯ç”¨å†…å­˜\n",
        "- **GPU é…ç½®**: æ˜¾å¡å‹å·å’Œæ˜¾å­˜å¤§å°\n",
        "- **CUDA ç‰ˆæœ¬**: æ·±åº¦å­¦ä¹ æ¡†æ¶æ”¯æŒ\n",
        "- **Python ç¯å¢ƒ**: è§£é‡Šå™¨ç‰ˆæœ¬\n",
        "- **ç£ç›˜ç©ºé—´**: å¯ç”¨å­˜å‚¨ç©ºé—´"
      ],
      "metadata": {
        "id": "SnR5kLU5jPot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ” ç¯å¢ƒä¿¡æ¯æ£€æŸ¥è„šæœ¬\n",
        "#\n",
        "# æœ¬è„šæœ¬çš„ä½œç”¨ï¼š\n",
        "# 1. å®‰è£… pandas åº“ç”¨äºæ•°æ®è¡¨æ ¼å±•ç¤º\n",
        "# 2. æ£€æŸ¥ç³»ç»Ÿçš„å„é¡¹é…ç½®ä¿¡æ¯\n",
        "# 3. ç”Ÿæˆè¯¦ç»†çš„ç¯å¢ƒæŠ¥å‘Šè¡¨æ ¼\n",
        "#\n",
        "# å¯¹äºåˆå­¦è€…æ¥è¯´ï¼Œè¿™ä¸ªæ­¥éª¤å¸®åŠ©æ‚¨ï¼š\n",
        "# - äº†è§£å½“å‰è¿è¡Œç¯å¢ƒçš„ç¡¬ä»¶é…ç½®\n",
        "# - ç¡®è®¤æ˜¯å¦æ»¡è¶³æ¨¡å‹è¿è¡Œçš„æœ€ä½è¦æ±‚\n",
        "# - å­¦ä¹ å¦‚ä½•é€šè¿‡ä»£ç è·å–ç³»ç»Ÿä¿¡æ¯\n",
        "\n",
        "# å®‰è£… pandas åº“ - ç”¨äºåˆ›å»ºå’Œå±•ç¤ºæ•°æ®è¡¨æ ¼\n",
        "# pandas æ˜¯ Python ä¸­æœ€æµè¡Œçš„æ•°æ®å¤„ç†å’Œåˆ†æåº“\n",
        "!pip install pandas==2.2.2\n",
        "\n",
        "import platform # å¯¼å…¥ platform æ¨¡å—ä»¥è·å–ç³»ç»Ÿä¿¡æ¯\n",
        "import os # å¯¼å…¥ os æ¨¡å—ä»¥ä¸æ“ä½œç³»ç»Ÿäº¤äº’\n",
        "import subprocess # å¯¼å…¥ subprocess æ¨¡å—ä»¥è¿è¡Œå¤–éƒ¨å‘½ä»¤\n",
        "import pandas as pd # å¯¼å…¥ pandas æ¨¡å—ï¼Œé€šå¸¸ç”¨äºæ•°æ®å¤„ç†ï¼Œè¿™é‡Œç”¨äºåˆ›å»ºè¡¨æ ¼\n",
        "import shutil # å¯¼å…¥ shutil æ¨¡å—ä»¥è·å–ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
        "\n",
        "# è·å– CPU ä¿¡æ¯çš„å‡½æ•°ï¼ŒåŒ…æ‹¬æ ¸å¿ƒæ•°é‡\n",
        "def get_cpu_info():\n",
        "    cpu_info = \"\" # åˆå§‹åŒ– CPU ä¿¡æ¯å­—ç¬¦ä¸²\n",
        "    physical_cores = \"N/A\"\n",
        "    logical_cores = \"N/A\"\n",
        "\n",
        "    if platform.system() == \"Windows\": # å¦‚æœæ˜¯ Windows ç³»ç»Ÿ\n",
        "        cpu_info = platform.processor() # ä½¿ç”¨ platform.processor() è·å– CPU ä¿¡æ¯\n",
        "        try:\n",
        "            # è·å– Windows ä¸Šçš„æ ¸å¿ƒæ•°é‡ (éœ€è¦ WMI)\n",
        "            import wmi\n",
        "            c = wmi.WMI()\n",
        "            for proc in c.Win32_Processor():\n",
        "                physical_cores = proc.NumberOfCores\n",
        "                logical_cores = proc.NumberOfLogicalProcessors\n",
        "        except:\n",
        "            pass # å¦‚æœ WMI ä¸å¯ç”¨ï¼Œå¿½ç•¥é”™è¯¯\n",
        "\n",
        "    elif platform.system() == \"Darwin\": # å¦‚æœæ˜¯ macOS ç³»ç»Ÿ\n",
        "        # åœ¨ macOS ä¸Šä½¿ç”¨ sysctl å‘½ä»¤è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
        "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # æ›´æ–° PATH ç¯å¢ƒå˜é‡\n",
        "        try:\n",
        "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            stdout_brand, stderr_brand = process_brand.communicate()\n",
        "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
        "\n",
        "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            stdout_physical, stderr_physical = process_physical.communicate()\n",
        "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
        "\n",
        "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            stdout_logical, stderr_logical = process_logical.communicate()\n",
        "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
        "\n",
        "        except:\n",
        "            cpu_info = \"Could not retrieve CPU info\"\n",
        "            physical_cores = \"N/A\"\n",
        "            logical_cores = \"N/A\"\n",
        "\n",
        "    else:  # Linux ç³»ç»Ÿ\n",
        "        try:\n",
        "            # åœ¨ Linux ä¸Šè¯»å– /proc/cpuinfo æ–‡ä»¶è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
        "            with open('/proc/cpuinfo') as f:\n",
        "                physical_cores_count = 0\n",
        "                logical_cores_count = 0\n",
        "                cpu_info_lines = []\n",
        "                for line in f:\n",
        "                    if line.startswith('model name'): # æŸ¥æ‰¾ä»¥ 'model name'å¼€å¤´çš„è¡Œ\n",
        "                        if not cpu_info: # åªè·å–ç¬¬ä¸€ä¸ª model name\n",
        "                            cpu_info = line.split(': ')[1].strip()\n",
        "                    elif line.startswith('cpu cores'): # æŸ¥æ‰¾ä»¥ 'cpu cores' å¼€å¤´çš„è¡Œ\n",
        "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
        "                    elif line.startswith('processor'): # æŸ¥æ‰¾ä»¥ 'processor' å¼€å¤´çš„è¡Œ\n",
        "                        logical_cores_count += 1\n",
        "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
        "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
        "                if not cpu_info:\n",
        "                     cpu_info = \"Could not retrieve CPU info\"\n",
        "\n",
        "        except:\n",
        "            cpu_info = \"Could not retrieve CPU info\"\n",
        "            physical_cores = \"N/A\"\n",
        "            logical_cores = \"N/A\"\n",
        "\n",
        "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # è¿”å› CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
        "\n",
        "\n",
        "# è·å–å†…å­˜ä¿¡æ¯çš„å‡½æ•°\n",
        "def get_memory_info():\n",
        "    mem_info = \"\" # åˆå§‹åŒ–å†…å­˜ä¿¡æ¯å­—ç¬¦ä¸²\n",
        "    if platform.system() == \"Windows\":\n",
        "        # åœ¨ Windows ä¸Šä¸å®¹æ˜“é€šè¿‡æ ‡å‡†åº“è·å–ï¼Œéœ€è¦å¤–éƒ¨åº“æˆ– PowerShell\n",
        "        mem_info = \"Requires external tools on Windows\" # è®¾ç½®æç¤ºä¿¡æ¯\n",
        "    elif platform.system() == \"Darwin\": # å¦‚æœæ˜¯ macOS ç³»ç»Ÿ\n",
        "        # åœ¨ macOS ä¸Šä½¿ç”¨ sysctl å‘½ä»¤è·å–å†…å­˜å¤§å°\n",
        "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # è¿è¡Œ sysctl å‘½ä»¤\n",
        "        stdout, stderr = process.communicate() # è·å–æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯\n",
        "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # è§£æè¾“å‡ºï¼Œè·å–å†…å­˜å¤§å°ï¼ˆå­—èŠ‚ï¼‰\n",
        "        mem_gb = mem_bytes / (1024**3) # è½¬æ¢ä¸º GB\n",
        "        mem_info = f\"{mem_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡º\n",
        "    else:  # Linux ç³»ç»Ÿ\n",
        "        try:\n",
        "            # åœ¨ Linux ä¸Šè¯»å– /proc/meminfo æ–‡ä»¶è·å–å†…å­˜ä¿¡æ¯\n",
        "            with open('/proc/meminfo') as f:\n",
        "                total_mem_kb = 0\n",
        "                available_mem_kb = 0\n",
        "                for line in f:\n",
        "                    if line.startswith('MemTotal'): # æŸ¥æ‰¾ä»¥ 'MemTotal' å¼€å¤´çš„è¡Œ\n",
        "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # è§£æè¡Œï¼Œè·å–æ€»å†…å­˜ï¼ˆKBï¼‰\n",
        "                    elif line.startswith('MemAvailable'): # æŸ¥æ‰¾ä»¥ 'MemAvailable' å¼€å¤´çš„è¡Œ\n",
        "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # è§£æè¡Œï¼Œè·å–å¯ç”¨å†…å­˜ï¼ˆKBï¼‰\n",
        "\n",
        "                if total_mem_kb > 0:\n",
        "                    total_mem_gb = total_mem_kb / (1024**2) # è½¬æ¢ä¸º GB\n",
        "                    mem_info = f\"{total_mem_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡ºæ€»å†…å­˜\n",
        "                    if available_mem_kb > 0:\n",
        "                        available_mem_gb = available_mem_kb / (1024**2)\n",
        "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # æ·»åŠ å¯ç”¨å†…å­˜ä¿¡æ¯\n",
        "                else:\n",
        "                     mem_info = \"Could not retrieve memory info\" # å¦‚æœè¯»å–æ–‡ä»¶å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
        "\n",
        "        except:\n",
        "            mem_info = \"Could not retrieve memory info\" # å¦‚æœè¯»å–æ–‡ä»¶å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
        "    return mem_info # è¿”å›å†…å­˜ä¿¡æ¯\n",
        "\n",
        "# è·å– GPU ä¿¡æ¯çš„å‡½æ•°ï¼ŒåŒ…æ‹¬æ˜¾å­˜\n",
        "def get_gpu_info():\n",
        "    try:\n",
        "        # å°è¯•ä½¿ç”¨ nvidia-smi è·å– NVIDIA GPU ä¿¡æ¯å’Œæ˜¾å­˜\n",
        "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
        "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
        "            gpu_lines = result.stdout.strip().split('\\n') # è§£æè¾“å‡ºï¼Œè·å– GPU åç§°å’Œæ˜¾å­˜\n",
        "            gpu_info_list = []\n",
        "            for line in gpu_lines:\n",
        "                name, memory = line.split(', ')\n",
        "                gpu_info_list.append(f\"{name} ({memory})\") # æ ¼å¼åŒ– GPU ä¿¡æ¯\n",
        "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # è¿”å› GPU ä¿¡æ¯æˆ–æç¤ºä¿¡æ¯\n",
        "        else:\n",
        "             # å°è¯•ä½¿ç”¨ lshw è·å–å…¶ä»– GPU ä¿¡æ¯ (éœ€è¦å®‰è£… lshw)\n",
        "            try:\n",
        "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
        "                if result_lshw.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
        "                     # ç®€å•è§£æè¾“å‡ºä¸­çš„ product åç§°å’Œæ˜¾å­˜\n",
        "                    gpu_info_lines = []\n",
        "                    current_gpu = {}\n",
        "                    for line in result_lshw.stdout.splitlines():\n",
        "                        if 'product:' in line:\n",
        "                             if current_gpu:\n",
        "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
        "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
        "                        elif 'size:' in line and 'memory' in line:\n",
        "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
        "\n",
        "                    if current_gpu: # æ·»åŠ æœ€åä¸€ä¸ª GPU çš„ä¿¡æ¯\n",
        "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
        "\n",
        "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # å¦‚æœæ‰¾åˆ° GPU ä½†ä¿¡æ¯æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
        "                else:\n",
        "                    return \"No GPU found (checked nvidia-smi and lshw)\" # å¦‚æœä¸¤ä¸ªå‘½ä»¤éƒ½æ‰¾ä¸åˆ° GPUï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
        "            except FileNotFoundError:\n",
        "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # å¦‚æœæ‰¾ä¸åˆ° lshw å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
        "    except FileNotFoundError:\n",
        "        return \"No GPU found (nvidia-smi not found)\" # å¦‚æœæ‰¾ä¸åˆ° nvidia-smi å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
        "\n",
        "\n",
        "# è·å– CUDA ç‰ˆæœ¬çš„å‡½æ•°\n",
        "def get_cuda_version():\n",
        "    try:\n",
        "        # å°è¯•ä½¿ç”¨ nvcc --version è·å– CUDA ç‰ˆæœ¬\n",
        "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
        "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
        "            for line in result.stdout.splitlines():\n",
        "                if 'release' in line: # æŸ¥æ‰¾åŒ…å« 'release' çš„è¡Œ\n",
        "                    return line.split('release ')[1].split(',')[0] # è§£æè¡Œï¼Œæå–ç‰ˆæœ¬å·\n",
        "        return \"CUDA not found or version not parsed\" # å¦‚æœæ‰¾ä¸åˆ° CUDA æˆ–ç‰ˆæœ¬æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
        "    except FileNotFoundError:\n",
        "        return \"CUDA not found\" # å¦‚æœæ‰¾ä¸åˆ° nvcc å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
        "\n",
        "# è·å– Python ç‰ˆæœ¬çš„å‡½æ•°\n",
        "def get_python_version():\n",
        "    return platform.python_version() # è·å– Python ç‰ˆæœ¬\n",
        "\n",
        "# è·å– Conda ç‰ˆæœ¬çš„å‡½æ•°\n",
        "def get_conda_version():\n",
        "    try:\n",
        "        # å°è¯•ä½¿ç”¨ conda --version è·å– Conda ç‰ˆæœ¬\n",
        "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
        "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
        "            return result.stdout.strip() # è¿”å› Conda ç‰ˆæœ¬\n",
        "        return \"Conda not found or version not parsed\" # å¦‚æœæ‰¾ä¸åˆ° Conda æˆ–ç‰ˆæœ¬æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
        "    except FileNotFoundError:\n",
        "        return \"Conda not found\" # å¦‚æœæ‰¾ä¸åˆ° conda å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
        "\n",
        "# è·å–ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯çš„å‡½æ•°\n",
        "def get_disk_space():\n",
        "    try:\n",
        "        total, used, free = shutil.disk_usage(\"/\") # è·å–æ ¹ç›®å½•çš„ç£ç›˜ä½¿ç”¨æƒ…å†µ\n",
        "        total_gb = total / (1024**3) # è½¬æ¢ä¸º GB\n",
        "        used_gb = used / (1024**3) # è½¬æ¢ä¸º GB\n",
        "        free_gb = free / (1024**3) # è½¬æ¢ä¸º GB\n",
        "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡º\n",
        "    except Exception as e:\n",
        "        return f\"Could not retrieve disk info: {e}\" # å¦‚æœè·å–ä¿¡æ¯å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
        "\n",
        "# è·å–ç¯å¢ƒä¿¡æ¯\n",
        "os_name = platform.system() # è·å–æ“ä½œç³»ç»Ÿåç§°\n",
        "os_version = platform.release() # è·å–æ“ä½œç³»ç»Ÿç‰ˆæœ¬\n",
        "if os_name == \"Linux\":\n",
        "    try:\n",
        "        # åœ¨ Linux ä¸Šå°è¯•è·å–å‘è¡Œç‰ˆå’Œç‰ˆæœ¬\n",
        "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
        "        if lsb_info.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
        "            for line in lsb_info.stdout.splitlines():\n",
        "                if 'Description:' in line: # æŸ¥æ‰¾åŒ…å« 'Description:' çš„è¡Œ\n",
        "                    os_version = line.split('Description:')[1].strip() # æå–æè¿°ä¿¡æ¯ä½œä¸ºç‰ˆæœ¬\n",
        "                    break # æ‰¾åˆ°åé€€å‡ºå¾ªç¯\n",
        "                elif 'Release:' in line: # æŸ¥æ‰¾åŒ…å« 'Release:' çš„è¡Œ\n",
        "                     os_version = line.split('Release:')[1].strip() # æå–ç‰ˆæœ¬å·\n",
        "                     # å°è¯•è·å– codename\n",
        "                     try:\n",
        "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
        "                         if codename_info.returncode == 0:\n",
        "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # å°† codename æ·»åŠ åˆ°ç‰ˆæœ¬ä¿¡æ¯ä¸­\n",
        "                     except:\n",
        "                         pass # å¦‚æœè·å– codename å¤±è´¥åˆ™å¿½ç•¥\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        pass # lsb_release å¯èƒ½æœªå®‰è£…ï¼Œå¿½ç•¥é”™è¯¯\n",
        "\n",
        "full_os_info = f\"{os_name} {os_version}\" # ç»„åˆå®Œæ•´çš„æ“ä½œç³»ç»Ÿä¿¡æ¯\n",
        "cpu_info = get_cpu_info() # è°ƒç”¨å‡½æ•°è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
        "memory_info = get_memory_info() # è°ƒç”¨å‡½æ•°è·å–å†…å­˜ä¿¡æ¯\n",
        "gpu_info = get_gpu_info() # è°ƒç”¨å‡½æ•°è·å– GPU ä¿¡æ¯å’Œæ˜¾å­˜\n",
        "cuda_version = get_cuda_version() # è°ƒç”¨å‡½æ•°è·å– CUDA ç‰ˆæœ¬\n",
        "python_version = get_python_version() # è°ƒç”¨å‡½æ•°è·å– Python ç‰ˆæœ¬\n",
        "conda_version = get_conda_version() # è°ƒç”¨å‡½æ•°è·å– Conda ç‰ˆæœ¬\n",
        "disk_info = get_disk_space() # è°ƒç”¨å‡½æ•°è·å–ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
        "\n",
        "\n",
        "# åˆ›å»ºç”¨äºå­˜å‚¨æ•°æ®çš„å­—å…¸\n",
        "env_data = {\n",
        "    \"é¡¹ç›®\": [ # é¡¹ç›®åç§°åˆ—è¡¨\n",
        "        \"æ“ä½œç³»ç»Ÿ\",\n",
        "        \"CPU ä¿¡æ¯\",\n",
        "        \"å†…å­˜ä¿¡æ¯\",\n",
        "        \"GPU ä¿¡æ¯\",\n",
        "        \"CUDA ä¿¡æ¯\",\n",
        "        \"Python ç‰ˆæœ¬\",\n",
        "        \"Conda ç‰ˆæœ¬\",\n",
        "        \"ç‰©ç†ç£ç›˜ç©ºé—´\" # æ·»åŠ ç‰©ç†ç£ç›˜ç©ºé—´\n",
        "    ],\n",
        "    \"ä¿¡æ¯\": [ # å¯¹åº”çš„ä¿¡æ¯åˆ—è¡¨\n",
        "        full_os_info,\n",
        "        cpu_info,\n",
        "        memory_info,\n",
        "        gpu_info,\n",
        "        cuda_version,\n",
        "        python_version,\n",
        "        conda_version,\n",
        "        disk_info # æ·»åŠ ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
        "    ]\n",
        "}\n",
        "\n",
        "# åˆ›å»ºä¸€ä¸ª pandas DataFrame\n",
        "df = pd.DataFrame(env_data)\n",
        "\n",
        "# æ‰“å°è¡¨æ ¼\n",
        "print(\"### ç¯å¢ƒä¿¡æ¯\") # æ‰“å°æ ‡é¢˜\n",
        "print(df.to_markdown(index=False)) # å°† DataFrame è½¬æ¢ä¸º Markdown æ ¼å¼å¹¶æ‰“å°ï¼Œä¸åŒ…å«ç´¢å¼•"
      ],
      "metadata": {
        "id": "LCDKeD2Oi6f8",
        "outputId": "f5d02626-cfc0-44e6-b0d5-13d8dba7f992",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
            "### ç¯å¢ƒä¿¡æ¯\n",
            "| é¡¹ç›®         | ä¿¡æ¯                                                               |\n",
            "|:-------------|:-------------------------------------------------------------------|\n",
            "| æ“ä½œç³»ç»Ÿ     | Linux Ubuntu 22.04.4 LTS                                           |\n",
            "| CPU ä¿¡æ¯     | Intel(R) Xeon(R) CPU @ 2.20GHz (1 physical cores, 2 logical cores) |\n",
            "| å†…å­˜ä¿¡æ¯     | 12.67 GB (Available: 10.44 GB)                                     |\n",
            "| GPU ä¿¡æ¯     | No GPU found (nvidia-smi not found)                                |\n",
            "| CUDA ä¿¡æ¯    | 12.5                                                               |\n",
            "| Python ç‰ˆæœ¬  | 3.11.13                                                            |\n",
            "| Conda ç‰ˆæœ¬   | Conda not found                                                    |\n",
            "| ç‰©ç†ç£ç›˜ç©ºé—´ | Total: 107.72 GB, Used: 37.81 GB, Free: 69.88 GB                   |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“¦ ç¬¬äºŒæ­¥ï¼šå®‰è£…ä¾èµ–åŒ…\n",
        "\n",
        "ç°åœ¨æˆ‘ä»¬éœ€è¦å®‰è£…è¿è¡Œæ¨¡å‹æ‰€éœ€çš„å…³é”® Python åŒ…ï¼š"
      ],
      "metadata": {
        "id": "fZ8QQTREjOkH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dY6Yd0lxhTFU",
        "outputId": "7d5abdae-dfaf-4ca3-bc40-16cdc79aa116",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, evaluate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed evaluate-0.4.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "# Transformers å®‰è£…\n",
        "! pip install transformers datasets evaluate accelerate\n",
        "# å¦‚æœè¦ä»æºç å®‰è£…è€Œä¸æ˜¯æœ€æ–°å‘å¸ƒç‰ˆæœ¬ï¼Œè¯·æ³¨é‡Šä¸Šé¢çš„å‘½ä»¤å¹¶å–æ¶ˆæ³¨é‡Šä¸‹é¢çš„å‘½ä»¤ã€‚\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOOKsqLzhTFV"
      },
      "source": [
        "# æ–‡æœ¬ç”Ÿæˆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQGWT9pchTFW"
      },
      "source": [
        "æ–‡æœ¬ç”Ÿæˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ€å—æ¬¢è¿çš„åº”ç”¨ã€‚å¤§è¯­è¨€æ¨¡å‹ç»è¿‡è®­ç»ƒï¼Œèƒ½å¤Ÿæ ¹æ®ç»™å®šçš„åˆå§‹æ–‡æœ¬ï¼ˆæç¤ºè¯ï¼‰ä»¥åŠå…¶è‡ªèº«ç”Ÿæˆçš„è¾“å‡ºï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªè¯ï¼ˆtokenï¼‰ï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šä¹‰çš„é•¿åº¦æˆ–é‡åˆ°åºåˆ—ç»“æŸï¼ˆ`EOS`ï¼‰tokenã€‚\n",
        "\n",
        "åœ¨ Transformers ä¸­ï¼Œ[generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) API è´Ÿè´£å¤„ç†æ–‡æœ¬ç”Ÿæˆï¼Œå®ƒé€‚ç”¨äºæ‰€æœ‰å…·æœ‰ç”Ÿæˆèƒ½åŠ›çš„æ¨¡å‹ã€‚æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºä½¿ç”¨ [generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) è¿›è¡Œæ–‡æœ¬ç”Ÿæˆçš„åŸºç¡€çŸ¥è¯†ä»¥åŠä¸€äº›éœ€è¦é¿å…çš„å¸¸è§é™·é˜±ã€‚\n",
        "\n",
        "\n",
        "> [!TIP] æ‚¨ä¹Ÿå¯ä»¥ç›´æ¥ä»å‘½ä»¤è¡Œä¸æ¨¡å‹å¯¹è¯ã€‚([å‚è€ƒæ–‡æ¡£](https://huggingface.co/docs/transformers/main/en/./conversations.md#transformers-cli)) ``` transformers chat Qwen/Qwen2.5-0.5B-Instruct```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRAZwtVOhTFW"
      },
      "source": [
        "## é»˜è®¤ç”Ÿæˆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezzEjK1KhTFW"
      },
      "source": [
        "åœ¨å¼€å§‹ä¹‹å‰ï¼Œå»ºè®®å®‰è£… [bitsandbytes](https://hf.co/docs/bitsandbytes/index) æ¥é‡åŒ–è¶…å¤§æ¨¡å‹ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚\\n\\n```bash\\n!pip install -U transformers bitsandbytes\\n```\\né™¤äº†åŸºäº CUDA çš„ GPU å¤–ï¼ŒBitsandbytes è¿˜æ”¯æŒå¤šç§åç«¯ã€‚è¯·å‚è€ƒå¤šåç«¯å®‰è£…[æŒ‡å—](https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend)äº†è§£æ›´å¤šä¿¡æ¯ã€‚\\n\\nä½¿ç”¨ [from_pretrained()](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) åŠ è½½å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶æ·»åŠ ä»¥ä¸‹ä¸¤ä¸ªå‚æ•°æ¥å‡å°‘å†…å­˜éœ€æ±‚ã€‚\\n\\n- `device_map=\\\"auto\\\"` å¯ç”¨ Accelerate çš„[å¤§æ¨¡å‹æ¨ç†](https://huggingface.co/docs/transformers/main/en/./models#big-model-inference)åŠŸèƒ½ï¼Œç”¨äºè‡ªåŠ¨åˆå§‹åŒ–æ¨¡å‹éª¨æ¶å¹¶åœ¨æ‰€æœ‰å¯ç”¨è®¾å¤‡ä¸ŠåŠ è½½å’Œåˆ†å‘æ¨¡å‹æƒé‡ï¼Œä»æœ€å¿«çš„è®¾å¤‡ï¼ˆGPUï¼‰å¼€å§‹ã€‚\\n- `quantization_config` æ˜¯å®šä¹‰é‡åŒ–è®¾ç½®çš„é…ç½®å¯¹è±¡ã€‚æ­¤ç¤ºä¾‹ä½¿ç”¨ bitsandbytes ä½œä¸ºé‡åŒ–åç«¯ï¼ˆæœ‰å…³æ›´å¤šå¯ç”¨åç«¯ï¼Œè¯·å‚è§[é‡åŒ–](https://huggingface.co/docs/transformers/main/en/./quantization/overview)éƒ¨åˆ†ï¼‰ï¼Œå¹¶ä»¥ [4ä½](https://huggingface.co/docs/transformers/main/en/./quantization/bitsandbytes)ç²¾åº¦åŠ è½½æ¨¡å‹ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyoAz8pQhTFX"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\",device_map=\"auto\",quantization_config=quantization_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6n5hOS-hTFX"
      },
      "source": [
        "å¯¹è¾“å…¥è¿›è¡Œåˆ†è¯ï¼Œå¹¶å°† `padding_side()` å‚æ•°è®¾ç½®ä¸º `\\\"left\\\"`ï¼Œå› ä¸ºå¤§è¯­è¨€æ¨¡å‹æ²¡æœ‰è¢«è®­ç»ƒä»å¡«å……tokenç»§ç»­ç”Ÿæˆã€‚åˆ†è¯å™¨è¿”å›è¾“å…¥IDå’Œæ³¨æ„åŠ›æ©ç ã€‚\\n\\n> [!TIP]\\n> é€šè¿‡å‘åˆ†è¯å™¨ä¼ é€’å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œå¯ä»¥ä¸€æ¬¡å¤„ç†å¤šä¸ªæç¤ºè¯ã€‚æ‰¹é‡å¤„ç†è¾“å…¥å¯ä»¥æé«˜ååé‡ï¼Œä½†ä¼šç•¥å¾®å¢åŠ å»¶è¿Ÿå’Œå†…å­˜æ¶ˆè€—ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmbojuWOhTFX"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\\\"mistralai/Mistral-7B-v0.1\\\", padding_side=\\\"left\\\")\\nmodel_inputs = tokenizer([\\\"A list of colors: red, blue\\\"], return_tensors=\\\"pt\\\").to(\\\"cuda\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj8TOHW6hTFX"
      },
      "source": [
        "å°†è¾“å…¥ä¼ é€’ç»™ [generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) æ¥ç”Ÿæˆtokenï¼Œå¹¶ä½¿ç”¨ [batch_decode()](https://huggingface.co/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode) å°†ç”Ÿæˆçš„tokenè§£ç å›æ–‡æœ¬ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ePzVeehhTFX"
      },
      "outputs": [],
      "source": [
        "generated_ids = model.generate(**model_inputs)\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\\n\\\"A list of colors: red, blue, green, yellow, orange, purple, pink,\\\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKQTWvSXhTFY"
      },
      "source": [
        "## ç”Ÿæˆé…ç½®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSd7zTyXhTFY"
      },
      "source": [
        "æ‰€æœ‰ç”Ÿæˆè®¾ç½®éƒ½åŒ…å«åœ¨ [GenerationConfig](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig) ä¸­ã€‚åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œç”Ÿæˆè®¾ç½®æ¥è‡ª [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) çš„ `generation_config.json` æ–‡ä»¶ã€‚å½“æ¨¡å‹æ²¡æœ‰ä¿å­˜é…ç½®æ—¶ï¼Œä¼šä½¿ç”¨é»˜è®¤çš„è§£ç ç­–ç•¥ã€‚\\n\\né€šè¿‡ `generation_config` å±æ€§æ£€æŸ¥é…ç½®ã€‚å®ƒåªæ˜¾ç¤ºä¸é»˜è®¤é…ç½®ä¸åŒçš„å€¼ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯ `bos_token_id` å’Œ `eos_token_id`ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSRH2qGNhTFY"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\\"mistralai/Mistral-7B-v0.1\\\", device_map=\\\"auto\\\")\\nmodel.generation_config\\nGenerationConfig {\\n  \\\"bos_token_id\\\": 1,\\n  \\\"eos_token_id\\\": 2\\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQZRgvI_hTFY"
      },
      "source": [
        "æ‚¨å¯ä»¥é€šè¿‡è¦†ç›– [GenerationConfig](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig) ä¸­çš„å‚æ•°å’Œå€¼æ¥è‡ªå®šä¹‰ [generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate)ã€‚æœ‰å…³å¸¸ç”¨è°ƒæ•´å‚æ•°ï¼Œè¯·å‚è§[ä¸‹é¢çš„éƒ¨åˆ†](#common-options)ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIREFqj3hTFY"
      },
      "outputs": [],
      "source": [
        "# å¯ç”¨æŸæœç´¢é‡‡æ ·ç­–ç•¥\\nmodel.generate(**model_inputs, num_beams=4, do_sample=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1DIN4VlhTFY"
      },
      "source": [
        "[generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) ä¹Ÿå¯ä»¥é€šè¿‡å¤–éƒ¨åº“æˆ–è‡ªå®šä¹‰ä»£ç è¿›è¡Œæ‰©å±•ï¼š\\n1. `logits_processor` å‚æ•°æ¥å—è‡ªå®šä¹‰çš„ [LogitsProcessor](https://huggingface.co/docs/transformers/main/en/internal/generation_utils#transformers.LogitsProcessor) å®ä¾‹ï¼Œç”¨äºæ“ä½œä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡åˆ†å¸ƒï¼›\\n2. `stopping_criteria` å‚æ•°æ”¯æŒè‡ªå®šä¹‰çš„ [StoppingCriteria](https://huggingface.co/docs/transformers/main/en/internal/generation_utils#transformers.StoppingCriteria) æ¥åœæ­¢æ–‡æœ¬ç”Ÿæˆï¼›\\n3. å…¶ä»–è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•å¯ä»¥é€šè¿‡ `custom_generate` æ ‡å¿—åŠ è½½ï¼ˆ[æ–‡æ¡£](https://huggingface.co/docs/transformers/main/en/generation_strategies.md/#custom-decoding-methods)ï¼‰ã€‚\\n\\nè¯·å‚è€ƒ[ç”Ÿæˆç­–ç•¥](https://huggingface.co/docs/transformers/main/en/./generation_strategies)æŒ‡å—ï¼Œäº†è§£æ›´å¤šå…³äºæœç´¢ã€é‡‡æ ·å’Œè§£ç ç­–ç•¥çš„ä¿¡æ¯ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsqZn2HFhTFY"
      },
      "source": [
        "### ä¿å­˜"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thvnic79hTFY"
      },
      "source": [
        "åˆ›å»ºä¸€ä¸ª [GenerationConfig](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig) å®ä¾‹å¹¶æŒ‡å®šæ‚¨æƒ³è¦çš„è§£ç å‚æ•°ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3vNHZ6xhTFY"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, GenerationConfig\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\\"my_account/my_model\\\")\\ngeneration_config = GenerationConfig(\\n    max_new_tokens=50, do_sample=True, top_k=50, eos_token_id=model.config.eos_token_id\\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5kz2I_KhTFY"
      },
      "source": [
        "ä½¿ç”¨ [save_pretrained()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained) ä¿å­˜ç‰¹å®šçš„ç”Ÿæˆé…ç½®ï¼Œå¹¶å°† `push_to_hub` å‚æ•°è®¾ç½®ä¸º `True` ä»¥å°†å…¶ä¸Šä¼ åˆ° Hubã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJUtKshPhTFZ"
      },
      "outputs": [],
      "source": [
        "generation_config.save_pretrained(\\\"my_account/my_model\\\", push_to_hub=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umN_PKAkhTFZ"
      },
      "source": [
        "å°† `config_file_name` å‚æ•°ç•™ç©ºã€‚å½“åœ¨å•ä¸ªç›®å½•ä¸­å­˜å‚¨å¤šä¸ªç”Ÿæˆé…ç½®æ—¶ï¼Œåº”ä½¿ç”¨æ­¤å‚æ•°ã€‚å®ƒä¸ºæ‚¨æä¾›äº†ä¸€ç§æŒ‡å®šè¦åŠ è½½å“ªä¸ªç”Ÿæˆé…ç½®çš„æ–¹æ³•ã€‚æ‚¨å¯ä»¥ä¸ºä¸åŒçš„ç”Ÿæˆä»»åŠ¡åˆ›å»ºä¸åŒçš„é…ç½®ï¼ˆä½¿ç”¨é‡‡æ ·çš„åˆ›æ„æ–‡æœ¬ç”Ÿæˆã€ä½¿ç”¨æŸæœç´¢çš„æ‘˜è¦ç”Ÿæˆï¼‰ï¼Œä»¥ä¾¿ä¸å•ä¸ªæ¨¡å‹ä¸€èµ·ä½¿ç”¨ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHEV8GfDhTFZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\\"google-t5/t5-small\\\")\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\\\"google-t5/t5-small\\\")\\n\\ntranslation_generation_config = GenerationConfig(\\n    num_beams=4,\\n    early_stopping=True,\\n    decoder_start_token_id=0,\\n    eos_token_id=model.config.eos_token_id,\\n    pad_token=model.config.pad_token_id,\\n)\\n\\ntranslation_generation_config.save_pretrained(\\\"/tmp\\\", config_file_name=\\\"translation_generation_config.json\\\", push_to_hub=True)\\n\\ngeneration_config = GenerationConfig.from_pretrained(\\\"/tmp\\\", config_file_name=\\\"translation_generation_config.json\\\")\\ninputs = tokenizer(\\\"translate English to French: Configuration files are easy to use!\\\", return_tensors=\\\"pt\\\")\\noutputs = model.generate(**inputs, generation_config=generation_config)\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS7-zU8fhTFZ"
      },
      "source": [
        "## å¸¸ç”¨é€‰é¡¹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVdlq5F9hTFZ"
      },
      "source": [
        "[generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§ä¸”å¯é«˜åº¦è‡ªå®šä¹‰çš„å·¥å…·ã€‚è¿™å¯¹æ–°ç”¨æˆ·æ¥è¯´å¯èƒ½ä¼šæ„Ÿåˆ°å›°æƒ‘ã€‚æœ¬èŠ‚åŒ…å«äº†æ‚¨å¯ä»¥åœ¨ Transformers çš„å¤§å¤šæ•°æ–‡æœ¬ç”Ÿæˆå·¥å…·ä¸­å®šä¹‰çš„å¸¸ç”¨ç”Ÿæˆé€‰é¡¹åˆ—è¡¨ï¼š[generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate)ã€[GenerationConfig](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig)ã€`pipelines`ã€`chat` CLI ç­‰...\\n\\n| é€‰é¡¹åç§° | ç±»å‹ | ç®€åŒ–æè¿° |\\n|---|---|---|\\n| `max_new_tokens` | `int` | æ§åˆ¶æœ€å¤§ç”Ÿæˆé•¿åº¦ã€‚è¯·åŠ¡å¿…å®šä¹‰å®ƒï¼Œå› ä¸ºå®ƒé€šå¸¸é»˜è®¤ä¸ºä¸€ä¸ªè¾ƒå°çš„å€¼ã€‚ |\\n| `do_sample` | `bool` | å®šä¹‰ç”Ÿæˆæ˜¯å¦ä¼šé‡‡æ ·ä¸‹ä¸€ä¸ªtokenï¼ˆ`True`ï¼‰ï¼Œæˆ–è€…ä½¿ç”¨è´ªå¿ƒç­–ç•¥ï¼ˆ`False`ï¼‰ã€‚å¤§å¤šæ•°ç”¨ä¾‹åº”å°†æ­¤æ ‡å¿—è®¾ç½®ä¸º `True`ã€‚æŸ¥çœ‹[æ­¤æŒ‡å—](https://huggingface.co/docs/transformers/main/en/./generation_strategies.md)äº†è§£æ›´å¤šä¿¡æ¯ã€‚ |\\n| `temperature` | `float` | ä¸‹ä¸€ä¸ªé€‰æ‹©çš„tokençš„ä¸å¯é¢„æµ‹æ€§ã€‚é«˜å€¼ï¼ˆ`>0.8`ï¼‰é€‚åˆåˆ›æ„ä»»åŠ¡ï¼Œä½å€¼ï¼ˆä¾‹å¦‚ `<0.4`ï¼‰é€‚åˆéœ€è¦\\\"æ€è€ƒ\\\"çš„ä»»åŠ¡ã€‚éœ€è¦ `do_sample=True`ã€‚ |\\n| `num_beams` | `int` | å½“è®¾ç½®ä¸º `>1` æ—¶ï¼Œæ¿€æ´»æŸæœç´¢ç®—æ³•ã€‚æŸæœç´¢åœ¨åŸºäºè¾“å…¥çš„ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚æŸ¥çœ‹[æ­¤æŒ‡å—](https://huggingface.co/docs/transformers/main/en/./generation_strategies.md)äº†è§£æ›´å¤šä¿¡æ¯ã€‚ |\\n| `repetition_penalty` | `float` | å¦‚æœæ‚¨å‘ç°æ¨¡å‹ç»å¸¸é‡å¤è‡ªå·±ï¼Œè¯·å°†å…¶è®¾ç½®ä¸º `>1.0`ã€‚è¾ƒå¤§çš„å€¼ä¼šæ–½åŠ æ›´å¤§çš„æƒ©ç½šã€‚ |\\n| `eos_token_id` | `list[int]` | å°†å¯¼è‡´ç”Ÿæˆåœæ­¢çš„tokenã€‚é»˜è®¤å€¼é€šå¸¸æ˜¯å¥½çš„ï¼Œä½†æ‚¨å¯ä»¥æŒ‡å®šä¸åŒçš„tokenã€‚ |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmcK3zZehTFZ"
      },
      "source": [
        "## å¸¸è§é™·é˜±"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G3b3M2AhTFZ"
      },
      "source": [
        "ä¸‹é¢çš„éƒ¨åˆ†æ¶µç›–äº†æ‚¨åœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­å¯èƒ½é‡åˆ°çš„ä¸€äº›å¸¸è§é—®é¢˜ä»¥åŠå¦‚ä½•è§£å†³å®ƒä»¬ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3qbYlxahTFZ"
      },
      "source": [
        "### è¾“å‡ºé•¿åº¦"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsR5oEAGhTFZ"
      },
      "source": [
        "[generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) é»˜è®¤è¿”å›æœ€å¤š20ä¸ªtokenï¼Œé™¤éåœ¨æ¨¡å‹çš„ [GenerationConfig](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig) ä¸­å¦æœ‰æŒ‡å®šã€‚å¼ºçƒˆå»ºè®®ä½¿ç”¨ `max_new_tokens` å‚æ•°æ‰‹åŠ¨è®¾ç½®ç”Ÿæˆçš„tokenæ•°é‡æ¥æ§åˆ¶è¾“å‡ºé•¿åº¦ã€‚[ä»…è§£ç å™¨](https://hf.co/learn/nlp-course/chapter1/6?fw=pt)æ¨¡å‹è¿”å›åˆå§‹æç¤ºè¯ä»¥åŠç”Ÿæˆçš„tokenã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "steCJMfbhTFZ"
      },
      "outputs": [],
      "source": [
        "model_inputs = tokenizer([\\\"A sequence of numbers: 1, 2\\\"], return_tensors=\\\"pt\\\").to(\\\"cuda\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNK2w_nchTFa"
      },
      "source": [
        "**é»˜è®¤é•¿åº¦ï¼š**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSqGLQWlhTFa"
      },
      "outputs": [],
      "source": [
        "generated_ids = model.generate(**model_inputs)\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\\n# è¾“å‡º: 'A sequence of numbers: 1, 2, 3, 4, 5'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iS-AG4AhTFa"
      },
      "source": [
        "**ä½¿ç”¨ max_new_tokensï¼š**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxF1No3phTFa"
      },
      "outputs": [],
      "source": [
        "generated_ids = model.generate(**model_inputs, max_new_tokens=50)\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\\n# è¾“å‡º: 'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeSAhsfmhTFa"
      },
      "source": [
        "### è§£ç ç­–ç•¥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFPftKG-hTFa"
      },
      "source": [
        "[generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) ä¸­çš„é»˜è®¤è§£ç ç­–ç•¥æ˜¯*è´ªå¿ƒæœç´¢*ï¼Œå®ƒé€‰æ‹©ä¸‹ä¸€ä¸ªæœ€å¯èƒ½çš„tokenï¼Œé™¤éåœ¨æ¨¡å‹çš„ [GenerationConfig](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig) ä¸­å¦æœ‰æŒ‡å®šã€‚è™½ç„¶è¿™ç§è§£ç ç­–ç•¥åœ¨åŸºäºè¾“å…¥çš„ä»»åŠ¡ï¼ˆè½¬å½•ã€ç¿»è¯‘ï¼‰ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å¯¹äºæ›´å…·åˆ›æ„çš„ç”¨ä¾‹ï¼ˆæ•…äº‹å†™ä½œã€èŠå¤©åº”ç”¨ï¼‰æ¥è¯´å¹¶ä¸æ˜¯æœ€ä¼˜çš„ã€‚\\n\\nä¾‹å¦‚ï¼Œå¯ç”¨[å¤šé¡¹å¼é‡‡æ ·](https://huggingface.co/docs/transformers/main/en/./generation_strategies#multinomial-sampling)ç­–ç•¥æ¥ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„è¾“å‡ºã€‚è¯·å‚è€ƒ[ç”Ÿæˆç­–ç•¥](https://huggingface.co/docs/transformers/main/en/./generation_strategies)æŒ‡å—äº†è§£æ›´å¤šè§£ç ç­–ç•¥ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRJq2jRihTFa"
      },
      "outputs": [],
      "source": [
        "model_inputs = tokenizer([\\\"I am a cat.\\\"], return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\n\\n# è´ªå¿ƒæœç´¢\\ngenerated_ids = model.generate(**model_inputs)\\nprint(\\\"è´ªå¿ƒæœç´¢:\\\", tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])\\n\\n# å¤šé¡¹å¼é‡‡æ ·\\ngenerated_ids = model.generate(**model_inputs, do_sample=True)\\nprint(\\\"å¤šé¡¹å¼é‡‡æ ·:\\\", tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Ga1v9LhTFa"
      },
      "source": [
        "### å¡«å……æ–¹å‘"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhduPBS2hTFa"
      },
      "source": [
        "å¦‚æœè¾“å…¥é•¿åº¦ä¸åŒï¼Œéœ€è¦è¿›è¡Œå¡«å……ã€‚ä½†æ˜¯å¤§è¯­è¨€æ¨¡å‹æ²¡æœ‰è¢«è®­ç»ƒä»å¡«å……tokenç»§ç»­ç”Ÿæˆï¼Œè¿™æ„å‘³ç€ `padding_side()` å‚æ•°éœ€è¦è®¾ç½®ä¸ºè¾“å…¥çš„å·¦ä¾§ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_yzSmBChTFf"
      },
      "outputs": [],
      "source": [
        "# å³å¡«å……ï¼ˆé”™è¯¯ï¼‰\\ntokenizer_right = AutoTokenizer.from_pretrained(\\\"mistralai/Mistral-7B-v0.1\\\", padding_side=\\\"right\\\")\\nmodel_inputs = tokenizer_right([\\\"1, 2, 3\\\", \\\"A, B, C, D, E\\\"], padding=True, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\ngenerated_ids = model.generate(**model_inputs)\\nprint(\\\"å³å¡«å……ç»“æœ:\\\", tokenizer_right.batch_decode(generated_ids, skip_special_tokens=True)[0])\\n\\n# å·¦å¡«å……ï¼ˆæ­£ç¡®ï¼‰\\ntokenizer_left = AutoTokenizer.from_pretrained(\\\"mistralai/Mistral-7B-v0.1\\\", padding_side=\\\"left\\\")\\ntokenizer_left.pad_token = tokenizer_left.eos_token\\nmodel_inputs = tokenizer_left([\\\"1, 2, 3\\\", \\\"A, B, C, D, E\\\"], padding=True, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\ngenerated_ids = model.generate(**model_inputs)\\nprint(\\\"å·¦å¡«å……ç»“æœ:\\\", tokenizer_left.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiV3XjZ1hTFf"
      },
      "source": [
        "### æç¤ºè¯æ ¼å¼"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy4q8PXohTFf"
      },
      "source": [
        "æŸäº›æ¨¡å‹å’Œä»»åŠ¡æœŸæœ›ç‰¹å®šçš„è¾“å…¥æç¤ºè¯æ ¼å¼ï¼Œå¦‚æœæ ¼å¼ä¸æ­£ç¡®ï¼Œæ¨¡å‹ä¼šè¿”å›æ¬¡ä¼˜çš„è¾“å‡ºã€‚æ‚¨å¯ä»¥åœ¨[æç¤ºè¯å·¥ç¨‹](https://huggingface.co/docs/transformers/main/en/./tasks/prompting)æŒ‡å—ä¸­äº†è§£æ›´å¤šå…³äºæç¤ºè¯çš„ä¿¡æ¯ã€‚\\n\\nä¾‹å¦‚ï¼ŒèŠå¤©æ¨¡å‹æœŸæœ›è¾“å…¥ä¸º[èŠå¤©æ¨¡æ¿](https://huggingface.co/docs/transformers/main/en/./chat_templating)ã€‚æ‚¨çš„æç¤ºè¯åº”åŒ…å« `role` å’Œ `content` æ¥æŒ‡ç¤ºè°åœ¨å‚ä¸å¯¹è¯ã€‚å¦‚æœæ‚¨å°è¯•å°†æç¤ºè¯ä½œä¸ºå•ä¸ªå­—ç¬¦ä¸²ä¼ é€’ï¼Œæ¨¡å‹å¹¶ä¸æ€»æ˜¯è¿”å›é¢„æœŸçš„è¾“å‡ºã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iprxRuBlhTFg"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\\"HuggingFaceH4/zephyr-7b-alpha\\\")\\nmodel = AutoModelForCausalLM.from_pretrained(\\\"HuggingFaceH4/zephyr-7b-alpha\\\", device_map=\\\"auto\\\", load_in_4bit=True)\\n\\n# æ— æ ¼å¼\\nprompt = \\\"How many cats does it take to change a light bulb? Reply as a pirate.\\\"\\nmodel_inputs = tokenizer([prompt], return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\ninput_length = model_inputs.input_ids.shape[1]\\ngenerated_ids = model.generate(**model_inputs, max_new_tokens=50)\\nprint(\\\"æ— æ ¼å¼:\\\", tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])\\n\\n# èŠå¤©æ¨¡æ¿\\nmessages = [\\n    {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a friendly chatbot who always responds in the style of a pirate\\\"},\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"How many cats does it take to change a light bulb?\\\"}\\n]\\nmodel_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\ninput_length = model_inputs.shape[1]\\ngenerated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=50)\\nprint(\\\"èŠå¤©æ¨¡æ¿:\\\", tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkEGxFpKhTFg"
      },
      "source": [
        "## èµ„æº"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beYlBJjKhTFg"
      },
      "source": [
        "è¯·æŸ¥çœ‹ä¸‹é¢ä¸€äº›æ›´å…·ä½“å’Œä¸“ä¸šçš„æ–‡æœ¬ç”Ÿæˆåº“ã€‚\\n\\n- [Optimum](https://github.com/huggingface/optimum)ï¼šTransformers çš„æ‰©å±•ï¼Œä¸“æ³¨äºåœ¨ç‰¹å®šç¡¬ä»¶è®¾å¤‡ä¸Šä¼˜åŒ–è®­ç»ƒå’Œæ¨ç†\\n- [Outlines](https://github.com/dottxt-ai/outlines)ï¼šç”¨äºçº¦æŸæ–‡æœ¬ç”Ÿæˆçš„åº“ï¼ˆä¾‹å¦‚ç”Ÿæˆ JSON æ–‡ä»¶ï¼‰ã€‚\\n- [SynCode](https://github.com/uiuc-focal-lab/syncode)ï¼šç”¨äºä¸Šä¸‹æ–‡æ— å…³è¯­æ³•å¼•å¯¼ç”Ÿæˆçš„åº“ï¼ˆJSONã€SQLã€Pythonï¼‰ã€‚\\n- [Text Generation Inference](https://github.com/huggingface/text-generation-inference)ï¼šç”¨äºå¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿäº§å°±ç»ªæœåŠ¡å™¨ã€‚\\n- [Text generation web UI](https://github.com/oobabooga/text-generation-webui)ï¼šç”¨äºæ–‡æœ¬ç”Ÿæˆçš„ Gradio Web UIã€‚\\n- [logits-processor-zoo](https://github.com/NVIDIA/logits-processor-zoo)ï¼šç”¨äºæ§åˆ¶æ–‡æœ¬ç”Ÿæˆçš„é¢å¤– logits å¤„ç†å™¨ã€‚"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
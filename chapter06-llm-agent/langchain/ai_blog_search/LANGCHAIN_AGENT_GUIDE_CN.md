# LangChain Agentæ·±åº¦å­¦ä¹ æŒ‡å—

## ç›®å½•
1. [AgentåŸºç¡€æ¦‚å¿µ](#agentåŸºç¡€æ¦‚å¿µ)
2. [LangGraphå·¥ä½œåŸç†](#langgraphå·¥ä½œåŸç†)
3. [RAGç³»ç»Ÿæ¶æ„](#ragç³»ç»Ÿæ¶æ„)
4. [æ ¸å¿ƒç»„ä»¶è¯¦è§£](#æ ¸å¿ƒç»„ä»¶è¯¦è§£)
5. [å®æˆ˜æ¡ˆä¾‹åˆ†æ](#å®æˆ˜æ¡ˆä¾‹åˆ†æ)
6. [è¿›é˜¶å¼€å‘æŠ€å·§](#è¿›é˜¶å¼€å‘æŠ€å·§)
7. [å¸¸è§é—®é¢˜è§£ç­”](#å¸¸è§é—®é¢˜è§£ç­”)

---

## AgentåŸºç¡€æ¦‚å¿µ

### ä»€ä¹ˆæ˜¯Agentï¼ˆæ™ºèƒ½ä½“ï¼‰ï¼Ÿ

Agentæ˜¯ä¸€ä¸ªèƒ½å¤Ÿ**è‡ªä¸»æ„ŸçŸ¥ç¯å¢ƒã€åšå‡ºå†³ç­–å¹¶æ‰§è¡Œè¡ŒåŠ¨**çš„AIç³»ç»Ÿã€‚åœ¨LangChainä¸­ï¼ŒAgentå…·å¤‡ä»¥ä¸‹æ ¸å¿ƒèƒ½åŠ›ï¼š

#### ğŸ§  è®¤çŸ¥èƒ½åŠ›
- **ç†è§£è¾“å…¥**ï¼šåˆ†æç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾å’Œä¸Šä¸‹æ–‡
- **æ¨ç†å†³ç­–**ï¼šåŸºäºå½“å‰çŠ¶æ€é€‰æ‹©æœ€ä½³è¡ŒåŠ¨æ–¹æ¡ˆ
- **å­¦ä¹ é€‚åº”**ï¼šä»äº¤äº’ä¸­å­¦ä¹ ï¼Œä¼˜åŒ–åç»­å†³ç­–

#### ğŸ› ï¸ æ‰§è¡Œèƒ½åŠ›
- **å·¥å…·è°ƒç”¨**ï¼šèƒ½å¤Ÿè°ƒç”¨å¤–éƒ¨å·¥å…·å’ŒAPI
- **çŠ¶æ€ç®¡ç†**ï¼šç»´æŠ¤å¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡ä¿¡æ¯
- **é”™è¯¯å¤„ç†**ï¼šå¤„ç†å¼‚å¸¸æƒ…å†µå¹¶è¿›è¡Œæ¢å¤

#### ğŸ”„ äº¤äº’èƒ½åŠ›
- **å¤šè½®å¯¹è¯**ï¼šæ”¯æŒå¤æ‚çš„å¤šè½®äº¤äº’
- **ä¸Šä¸‹æ–‡ä¿æŒ**ï¼šåœ¨å¯¹è¯è¿‡ç¨‹ä¸­ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
- **åé¦ˆå­¦ä¹ **ï¼šæ ¹æ®ç”¨æˆ·åé¦ˆè°ƒæ•´è¡Œä¸º

### Agent vs ä¼ ç»Ÿç¨‹åºçš„åŒºåˆ«

| ç‰¹æ€§ | ä¼ ç»Ÿç¨‹åº | Agentç³»ç»Ÿ |
|------|----------|-----------|
| **å†³ç­–æ–¹å¼** | é¢„å®šä¹‰è§„åˆ™ | æ™ºèƒ½æ¨ç† |
| **é€‚åº”æ€§** | å›ºå®šé€»è¾‘ | åŠ¨æ€è°ƒæ•´ |
| **äº¤äº’æ¨¡å¼** | å•å‘æ‰§è¡Œ | åŒå‘å¯¹è¯ |
| **é”™è¯¯å¤„ç†** | å¼‚å¸¸ä¸­æ–­ | æ™ºèƒ½æ¢å¤ |
| **å­¦ä¹ èƒ½åŠ›** | æ— å­¦ä¹  | æŒç»­å­¦ä¹  |

### Agentçš„åº”ç”¨åœºæ™¯

#### ğŸ“š çŸ¥è¯†é—®ç­”ç³»ç»Ÿ
```python
# ç¤ºä¾‹ï¼šæ™ºèƒ½å®¢æœAgent
def customer_service_agent(query):
    """
    æ™ºèƒ½å®¢æœAgentç¤ºä¾‹
    èƒ½å¤Ÿç†è§£ç”¨æˆ·é—®é¢˜ï¼ŒæŸ¥è¯¢çŸ¥è¯†åº“ï¼Œæä¾›å‡†ç¡®ç­”æ¡ˆ
    """
    # 1. ç†è§£ç”¨æˆ·æ„å›¾
    intent = analyze_intent(query)
    
    # 2. é€‰æ‹©åˆé€‚çš„å·¥å…·
    if intent == "product_info":
        tool = product_database_tool
    elif intent == "order_status":
        tool = order_tracking_tool
    
    # 3. æ‰§è¡ŒæŸ¥è¯¢å¹¶ç”Ÿæˆå›ç­”
    result = tool.invoke(query)
    return generate_response(result)
```

#### ğŸ” ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿ
```python
# ç¤ºä¾‹ï¼šç ”ç©¶åŠ©æ‰‹Agent
def research_agent(topic):
    """
    ç ”ç©¶åŠ©æ‰‹Agentç¤ºä¾‹
    èƒ½å¤Ÿè‡ªåŠ¨æœç´¢ã€ç­›é€‰ã€æ€»ç»“ç›¸å…³ä¿¡æ¯
    """
    # 1. åˆ†è§£ç ”ç©¶ä»»åŠ¡
    subtasks = decompose_research_task(topic)
    
    # 2. å¹¶è¡Œæ‰§è¡Œæœç´¢
    results = []
    for subtask in subtasks:
        result = search_tool.invoke(subtask)
        results.append(result)
    
    # 3. ç»¼åˆåˆ†æå’Œæ€»ç»“
    summary = synthesize_results(results)
    return summary
```

---

## LangGraphå·¥ä½œåŸç†

### çŠ¶æ€å›¾ï¼ˆState Graphï¼‰æ¦‚å¿µ

LangGraphä½¿ç”¨**çŠ¶æ€å›¾**æ¥ç®¡ç†Agentçš„å·¥ä½œæµç¨‹ã€‚çŠ¶æ€å›¾æ˜¯ä¸€ç§æ•°å­¦æ¨¡å‹ï¼Œç”±ä»¥ä¸‹å…ƒç´ ç»„æˆï¼š

#### ğŸ”µ èŠ‚ç‚¹ï¼ˆNodesï¼‰
- **åŠŸèƒ½èŠ‚ç‚¹**ï¼šæ‰§è¡Œç‰¹å®šä»»åŠ¡çš„å¤„ç†å•å…ƒ
- **å†³ç­–èŠ‚ç‚¹**ï¼šæ ¹æ®æ¡ä»¶é€‰æ‹©ä¸‹ä¸€æ­¥è¡ŒåŠ¨
- **å·¥å…·èŠ‚ç‚¹**ï¼šè°ƒç”¨å¤–éƒ¨å·¥å…·å’ŒæœåŠ¡

#### â¡ï¸ è¾¹ï¼ˆEdgesï¼‰
- **å›ºå®šè¾¹**ï¼šç¡®å®šçš„æµç¨‹è½¬æ¢
- **æ¡ä»¶è¾¹**ï¼šåŸºäºæ¡ä»¶çš„åŠ¨æ€è·¯ç”±
- **å¾ªç¯è¾¹**ï¼šæ”¯æŒè¿­ä»£å’Œé‡è¯•æœºåˆ¶

#### ğŸ“Š çŠ¶æ€ï¼ˆStateï¼‰
- **å…¨å±€çŠ¶æ€**ï¼šåœ¨æ•´ä¸ªæµç¨‹ä¸­å…±äº«çš„æ•°æ®
- **å±€éƒ¨çŠ¶æ€**ï¼šèŠ‚ç‚¹å†…éƒ¨çš„ä¸´æ—¶æ•°æ®
- **å†å²çŠ¶æ€**ï¼šä¿å­˜çš„å†å²ä¿¡æ¯

### æœ¬é¡¹ç›®çš„çŠ¶æ€å›¾ç»“æ„

```mermaid
graph TD
    A[ç”¨æˆ·æŸ¥è¯¢] --> B[Agentå†³ç­–]
    B --> C{éœ€è¦æ£€ç´¢?}
    C -->|æ˜¯| D[æ–‡æ¡£æ£€ç´¢]
    C -->|å¦| E[ç›´æ¥ç»“æŸ]
    D --> F[ç›¸å…³æ€§è¯„ä¼°]
    F --> G{æ–‡æ¡£ç›¸å…³?}
    G -->|æ˜¯| H[ç”Ÿæˆç­”æ¡ˆ]
    G -->|å¦| I[é‡å†™æŸ¥è¯¢]
    I --> B
    H --> E
```

### çŠ¶æ€å®šä¹‰è¯¦è§£

```python
class AgentState(TypedDict):
    """
    AgentçŠ¶æ€å®šä¹‰
    
    è¿™ä¸ªç±»å®šä¹‰äº†åœ¨æ•´ä¸ªå·¥ä½œæµä¸­ä¼ é€’çš„çŠ¶æ€ä¿¡æ¯
    ä½¿ç”¨TypedDictç¡®ä¿ç±»å‹å®‰å…¨
    """
    messages: Annotated[Sequence[BaseMessage], add_messages]
    # messages: å¯¹è¯æ¶ˆæ¯åºåˆ—
    # - åŒ…å«ç”¨æˆ·è¾“å…¥ã€Agentå“åº”ã€å·¥å…·è°ƒç”¨ç»“æœç­‰
    # - ä½¿ç”¨add_messageså‡½æ•°å¤„ç†æ¶ˆæ¯æ·»åŠ é€»è¾‘
    # - æ”¯æŒä¸åŒç±»å‹çš„æ¶ˆæ¯ï¼šHumanMessage, AIMessage, ToolMessage
```

### å·¥ä½œæµæ‰§è¡Œæœºåˆ¶

#### 1. åˆå§‹åŒ–é˜¶æ®µ
```python
# åˆ›å»ºåˆå§‹çŠ¶æ€
initial_state = {
    "messages": [HumanMessage(content="ç”¨æˆ·æŸ¥è¯¢")]
}
```

#### 2. èŠ‚ç‚¹æ‰§è¡Œé˜¶æ®µ
```python
# æ¯ä¸ªèŠ‚ç‚¹æ¥æ”¶å½“å‰çŠ¶æ€ï¼Œè¿”å›çŠ¶æ€æ›´æ–°
def node_function(state):
    # å¤„ç†é€»è¾‘
    result = process(state)
    
    # è¿”å›çŠ¶æ€æ›´æ–°
    return {"messages": [result]}
```

#### 3. çŠ¶æ€åˆå¹¶é˜¶æ®µ
```python
# LangGraphè‡ªåŠ¨åˆå¹¶çŠ¶æ€æ›´æ–°
# ä½¿ç”¨add_messageså‡½æ•°å¤„ç†æ¶ˆæ¯åˆ—è¡¨çš„åˆå¹¶
new_state = merge_states(current_state, state_update)
```

---

## RAGç³»ç»Ÿæ¶æ„

### RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰åŸç†

RAGç»“åˆäº†**ä¿¡æ¯æ£€ç´¢**å’Œ**æ–‡æœ¬ç”Ÿæˆ**ä¸¤ä¸ªæ ¸å¿ƒæŠ€æœ¯ï¼š

#### ğŸ” æ£€ç´¢é˜¶æ®µï¼ˆRetrievalï¼‰
1. **æŸ¥è¯¢ç†è§£**ï¼šåˆ†æç”¨æˆ·æŸ¥è¯¢æ„å›¾
2. **å‘é‡åŒ–**ï¼šå°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
3. **ç›¸ä¼¼åº¦æœç´¢**ï¼šåœ¨å‘é‡æ•°æ®åº“ä¸­æ‰¾åˆ°ç›¸å…³æ–‡æ¡£
4. **ç»“æœæ’åº**ï¼šæŒ‰ç›¸å…³æ€§å¯¹æ£€ç´¢ç»“æœæ’åº

#### ğŸ¤– ç”Ÿæˆé˜¶æ®µï¼ˆGenerationï¼‰
1. **ä¸Šä¸‹æ–‡æ„å»º**ï¼šå°†æ£€ç´¢åˆ°çš„æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡
2. **æç¤ºæ„é€ **ï¼šç»“åˆæŸ¥è¯¢å’Œä¸Šä¸‹æ–‡æ„å»ºæç¤º
3. **æ¨¡å‹æ¨ç†**ï¼šä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
4. **åå¤„ç†**ï¼šå¯¹ç”Ÿæˆç»“æœè¿›è¡Œä¼˜åŒ–å’Œæ ¼å¼åŒ–

### æœ¬é¡¹ç›®çš„RAGæ¶æ„

```python
# RAGæµç¨‹ç¤ºæ„
def rag_pipeline(query):
    """å®Œæ•´çš„RAGå¤„ç†æµç¨‹"""
    
    # 1. æŸ¥è¯¢é¢„å¤„ç†
    processed_query = preprocess_query(query)
    
    # 2. å‘é‡æ£€ç´¢
    relevant_docs = vector_search(processed_query)
    
    # 3. ç›¸å…³æ€§è¿‡æ»¤
    filtered_docs = filter_relevant_docs(relevant_docs, query)
    
    # 4. ä¸Šä¸‹æ–‡æ„å»º
    context = build_context(filtered_docs)
    
    # 5. ç­”æ¡ˆç”Ÿæˆ
    answer = generate_answer(query, context)
    
    return answer
```

### å‘é‡æ•°æ®åº“ï¼ˆQdrantï¼‰é›†æˆ

#### ğŸ—„ï¸ æ•°æ®å­˜å‚¨ç»“æ„
```python
# æ–‡æ¡£å‘é‡å­˜å‚¨æ ¼å¼
document_vector = {
    "id": "unique_document_id",
    "vector": [0.1, 0.2, 0.3, ...],  # 768ç»´å‘é‡
    "payload": {
        "content": "æ–‡æ¡£å†…å®¹",
        "url": "æ¥æºURL",
        "chunk_index": 0,
        "metadata": {...}
    }
}
```

#### ğŸ” æ£€ç´¢æŸ¥è¯¢è¿‡ç¨‹
```python
def search_similar_documents(query_vector, top_k=5):
    """
    åœ¨Qdrantä¸­æœç´¢ç›¸ä¼¼æ–‡æ¡£
    
    Args:
        query_vector: æŸ¥è¯¢å‘é‡
        top_k: è¿”å›ç»“æœæ•°é‡
    
    Returns:
        List[Document]: ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
    """
    search_result = qdrant_client.search(
        collection_name="qdrant_db",
        query_vector=query_vector,
        limit=top_k,
        score_threshold=0.7  # ç›¸ä¼¼åº¦é˜ˆå€¼
    )
    
    return [
        Document(
            page_content=hit.payload["content"],
            metadata=hit.payload["metadata"]
        )
        for hit in search_result
    ]
```

---

## æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. æ–‡æ¡£åŠ è½½å™¨ï¼ˆWebBaseLoaderï¼‰

#### åŠŸèƒ½ç‰¹æ€§
- **å¤šæ ¼å¼æ”¯æŒ**ï¼šHTML, PDF, Markdownç­‰
- **æ™ºèƒ½è§£æ**ï¼šè‡ªåŠ¨æå–ä¸»è¦å†…å®¹
- **å…ƒæ•°æ®ä¿ç•™**ï¼šä¿å­˜URLã€æ ‡é¢˜ç­‰ä¿¡æ¯

#### ä½¿ç”¨ç¤ºä¾‹
```python
from langchain_community.document_loaders import WebBaseLoader

# åˆ›å»ºåŠ è½½å™¨
loader = WebBaseLoader("https://example.com/blog-post")

# åŠ è½½æ–‡æ¡£
documents = loader.load()

# æŸ¥çœ‹æ–‡æ¡£ç»“æ„
for doc in documents:
    print(f"å†…å®¹é•¿åº¦: {len(doc.page_content)}")
    print(f"å…ƒæ•°æ®: {doc.metadata}")
```

#### é«˜çº§é…ç½®
```python
# è‡ªå®šä¹‰åŠ è½½å™¨é…ç½®
loader = WebBaseLoader(
    web_paths=["https://example.com/blog1", "https://example.com/blog2"],
    bs_kwargs={
        "parse_only": bs4.SoupStrainer("div", {"class": "content"}),
        "features": "html.parser"
    },
    header_template={
        "User-Agent": "Mozilla/5.0 (compatible; BlogBot/1.0)"
    }
)
```

### 2. æ–‡æ¡£åˆ†å‰²å™¨ï¼ˆRecursiveCharacterTextSplitterï¼‰

#### åˆ†å‰²ç­–ç•¥
1. **é€’å½’åˆ†å‰²**ï¼šæŒ‰æ®µè½ã€å¥å­ã€è¯è¯­é€çº§åˆ†å‰²
2. **é‡å ä¿æŒ**ï¼šä¿æŒå—ä¹‹é—´çš„ä¸Šä¸‹æ–‡è¿ç»­æ€§
3. **é•¿åº¦æ§åˆ¶**ï¼šç¡®ä¿æ¯å—å¤§å°é€‚åˆæ¨¡å‹å¤„ç†

#### é…ç½®å‚æ•°è¯¦è§£
```python
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=1000,        # æ¯å—æœ€å¤§tokenæ•°
    chunk_overlap=200,      # é‡å tokenæ•°
    length_function=len,    # é•¿åº¦è®¡ç®—å‡½æ•°
    separators=[           # åˆ†å‰²ç¬¦ä¼˜å…ˆçº§
        "\n\n",           # æ®µè½åˆ†å‰²
        "\n",             # è¡Œåˆ†å‰²
        " ",              # è¯åˆ†å‰²
        ""                # å­—ç¬¦åˆ†å‰²
    ]
)
```

#### åˆ†å‰²æ•ˆæœç¤ºä¾‹
```python
# åŸå§‹æ–‡æ¡£
original_text = """
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚å®ƒè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚

æœºå™¨å­¦ä¹ æ˜¯AIçš„ä¸€ä¸ªå­é¢†åŸŸã€‚å®ƒä½¿ç”¨ç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹æ¥è®©è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›ã€‚

æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªç‰¹æ®Šåˆ†æ”¯ã€‚å®ƒä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚
"""

# åˆ†å‰²ç»“æœ
chunks = text_splitter.split_text(original_text)
# ç»“æœï¼š3ä¸ªé‡å çš„æ–‡æ¡£å—ï¼Œæ¯ä¸ªåŒ…å«å®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
```

### 3. åµŒå…¥æ¨¡å‹ï¼ˆGoogleGenerativeAIEmbeddingsï¼‰

#### æ¨¡å‹ç‰¹æ€§
- **é«˜ç»´å‘é‡**ï¼š768ç»´å‘é‡è¡¨ç¤º
- **è¯­ä¹‰ç†è§£**ï¼šæ•è·æ–‡æœ¬çš„æ·±å±‚è¯­ä¹‰
- **å¤šè¯­è¨€æ”¯æŒ**ï¼šæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰å¤šç§è¯­è¨€

#### å‘é‡åŒ–è¿‡ç¨‹
```python
# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
embedding_model = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key="your_api_key"
)

# æ–‡æœ¬å‘é‡åŒ–
text = "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ"
vector = embedding_model.embed_query(text)

print(f"å‘é‡ç»´åº¦: {len(vector)}")  # è¾“å‡º: 768
print(f"å‘é‡ç¤ºä¾‹: {vector[:5]}")   # æ˜¾ç¤ºå‰5ä¸ªç»´åº¦
```

#### æ‰¹é‡å¤„ç†ä¼˜åŒ–
```python
# æ‰¹é‡å‘é‡åŒ–ï¼ˆæé«˜æ•ˆç‡ï¼‰
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3", ...]
vectors = embedding_model.embed_documents(texts)

# å¼‚æ­¥å¤„ç†ï¼ˆé€‚ç”¨äºå¤§é‡æ–‡æ¡£ï¼‰
import asyncio

async def embed_documents_async(texts):
    tasks = [embedding_model.aembed_query(text) for text in texts]
    return await asyncio.gather(*tasks)
```

### 4. å¯¹è¯æ¨¡å‹ï¼ˆChatGoogleGenerativeAIï¼‰

#### æ¨¡å‹é…ç½®
```python
chat_model = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    api_key="your_api_key",
    temperature=0.1,        # æ§åˆ¶éšæœºæ€§
    max_tokens=2048,        # æœ€å¤§è¾“å‡ºé•¿åº¦
    top_p=0.9,             # æ ¸é‡‡æ ·å‚æ•°
    top_k=40,              # Top-Ké‡‡æ ·å‚æ•°
    streaming=True         # æµå¼è¾“å‡º
)
```

#### æ¶ˆæ¯ç±»å‹å¤„ç†
```python
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# æ„å»ºå¯¹è¯å†å²
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹"),
    HumanMessage(content="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"),
    AIMessage(content="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯..."),
    HumanMessage(content="è¯·ä¸¾ä¸ªå…·ä½“ä¾‹å­")
]

# ç”Ÿæˆå›å¤
response = chat_model.invoke(messages)
```

#### å·¥å…·ç»‘å®šæœºåˆ¶
```python
# å®šä¹‰å·¥å…·
def search_tool(query: str) -> str:
    """æœç´¢å·¥å…·"""
    return f"æœç´¢ç»“æœ: {query}"

# ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
model_with_tools = chat_model.bind_tools([search_tool])

# æ¨¡å‹ä¼šè‡ªåŠ¨å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
response = model_with_tools.invoke([
    HumanMessage(content="å¸®æˆ‘æœç´¢æœ€æ–°çš„AIæ–°é—»")
])
```

---

## å®æˆ˜æ¡ˆä¾‹åˆ†æ

### æ¡ˆä¾‹1ï¼šæŸ¥è¯¢ç†è§£ä¸é‡å†™

#### åœºæ™¯æè¿°
ç”¨æˆ·è¾“å…¥æ¨¡ç³ŠæŸ¥è¯¢ï¼š"è¿™ä¸ªä¸œè¥¿æ€ä¹ˆç”¨ï¼Ÿ"

#### å¤„ç†æµç¨‹
```python
def handle_ambiguous_query(query, context):
    """å¤„ç†æ¨¡ç³ŠæŸ¥è¯¢"""
    
    # 1. åˆ†ææŸ¥è¯¢æ¨¡ç³Šæ€§
    ambiguity_score = analyze_ambiguity(query)
    
    if ambiguity_score > 0.7:  # é«˜åº¦æ¨¡ç³Š
        # 2. è¯·æ±‚æ¾„æ¸…
        clarification = request_clarification(query, context)
        return clarification
    
    elif ambiguity_score > 0.4:  # ä¸­åº¦æ¨¡ç³Š
        # 3. æ™ºèƒ½é‡å†™
        rewritten_query = rewrite_query(query, context)
        return process_query(rewritten_query)
    
    else:  # æ¸…æ™°æŸ¥è¯¢
        # 4. ç›´æ¥å¤„ç†
        return process_query(query)
```

#### é‡å†™ç¤ºä¾‹
```python
# åŸå§‹æŸ¥è¯¢
original_query = "è¿™ä¸ªä¸œè¥¿æ€ä¹ˆç”¨ï¼Ÿ"

# ä¸Šä¸‹æ–‡ä¿¡æ¯
context = {
    "previous_topic": "Pythonè£…é¥°å™¨",
    "document_content": "è£…é¥°å™¨æ˜¯Pythonçš„é«˜çº§ç‰¹æ€§..."
}

# é‡å†™ç»“æœ
rewritten_query = "Pythonè£…é¥°å™¨çš„ä½¿ç”¨æ–¹æ³•å’Œè¯­æ³•æ˜¯ä»€ä¹ˆï¼Ÿ"
```

### æ¡ˆä¾‹2ï¼šç›¸å…³æ€§è¯„ä¼°æœºåˆ¶

#### è¯„ä¼°æ ‡å‡†
```python
def evaluate_document_relevance(query, document):
    """
    æ–‡æ¡£ç›¸å…³æ€§è¯„ä¼°
    
    è¯„ä¼°ç»´åº¦ï¼š
    1. å…³é”®è¯åŒ¹é…åº¦
    2. è¯­ä¹‰ç›¸ä¼¼åº¦
    3. ä¸»é¢˜ä¸€è‡´æ€§
    4. ä¿¡æ¯å®Œæ•´æ€§
    """
    
    # 1. å…³é”®è¯åŒ¹é…
    keyword_score = calculate_keyword_overlap(query, document)
    
    # 2. è¯­ä¹‰ç›¸ä¼¼åº¦
    semantic_score = calculate_semantic_similarity(query, document)
    
    # 3. ä¸»é¢˜ä¸€è‡´æ€§
    topic_score = calculate_topic_consistency(query, document)
    
    # 4. ç»¼åˆè¯„åˆ†
    final_score = (
        keyword_score * 0.3 +
        semantic_score * 0.4 +
        topic_score * 0.3
    )
    
    return final_score > 0.6  # é˜ˆå€¼åˆ¤æ–­
```

#### è¯„ä¼°ç¤ºä¾‹
```python
# æŸ¥è¯¢
query = "å¦‚ä½•ä½¿ç”¨Pythonè¿›è¡Œæ•°æ®åˆ†æï¼Ÿ"

# æ–‡æ¡£1ï¼ˆé«˜ç›¸å…³æ€§ï¼‰
doc1 = "Pythonæ•°æ®åˆ†æå…¥é—¨ï¼šä½¿ç”¨pandaså’Œnumpyè¿›è¡Œæ•°æ®å¤„ç†..."
relevance1 = evaluate_document_relevance(query, doc1)  # True

# æ–‡æ¡£2ï¼ˆä½ç›¸å…³æ€§ï¼‰
doc2 = "JavaScriptå‰ç«¯å¼€å‘æŠ€å·§å’Œæœ€ä½³å®è·µ..."
relevance2 = evaluate_document_relevance(query, doc2)  # False
```

### æ¡ˆä¾‹3ï¼šå¤šè½®å¯¹è¯ç®¡ç†

#### å¯¹è¯çŠ¶æ€è·Ÿè¸ª
```python
class ConversationManager:
    """å¯¹è¯ç®¡ç†å™¨"""
    
    def __init__(self):
        self.history = []
        self.context = {}
        self.user_intent = None
    
    def add_message(self, message):
        """æ·»åŠ æ¶ˆæ¯åˆ°å†å²"""
        self.history.append(message)
        self.update_context(message)
    
    def update_context(self, message):
        """æ›´æ–°å¯¹è¯ä¸Šä¸‹æ–‡"""
        # æå–å®ä½“å’Œæ„å›¾
        entities = extract_entities(message.content)
        intent = classify_intent(message.content)
        
        # æ›´æ–°ä¸Šä¸‹æ–‡
        self.context.update(entities)
        self.user_intent = intent
    
    def get_relevant_history(self, max_turns=5):
        """è·å–ç›¸å…³å†å²å¯¹è¯"""
        return self.history[-max_turns:]
```

#### ä¸Šä¸‹æ–‡æ„ŸçŸ¥å›ç­”
```python
def generate_context_aware_answer(query, conversation_manager):
    """ç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å›ç­”"""
    
    # 1. è·å–å¯¹è¯å†å²
    history = conversation_manager.get_relevant_history()
    
    # 2. æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
    full_context = build_context_from_history(history)
    
    # 3. ç»“åˆå½“å‰æŸ¥è¯¢
    enhanced_query = f"""
    å¯¹è¯å†å²ï¼š{full_context}
    å½“å‰é—®é¢˜ï¼š{query}
    
    è¯·åŸºäºå¯¹è¯å†å²å›ç­”å½“å‰é—®é¢˜ã€‚
    """
    
    # 4. ç”Ÿæˆå›ç­”
    answer = chat_model.invoke([HumanMessage(content=enhanced_query)])
    
    return answer
```

---

## è¿›é˜¶å¼€å‘æŠ€å·§

### 1. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### ç¼“å­˜æœºåˆ¶
```python
from functools import lru_cache
import hashlib

class EmbeddingCache:
    """åµŒå…¥å‘é‡ç¼“å­˜"""
    
    def __init__(self, max_size=1000):
        self.cache = {}
        self.max_size = max_size
    
    def get_embedding(self, text):
        """è·å–ç¼“å­˜çš„åµŒå…¥å‘é‡"""
        text_hash = hashlib.md5(text.encode()).hexdigest()
        
        if text_hash in self.cache:
            return self.cache[text_hash]
        
        # è®¡ç®—æ–°çš„åµŒå…¥å‘é‡
        embedding = embedding_model.embed_query(text)
        
        # ç¼“å­˜ç®¡ç†
        if len(self.cache) >= self.max_size:
            # åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
        
        self.cache[text_hash] = embedding
        return embedding
```

#### æ‰¹å¤„ç†ä¼˜åŒ–
```python
async def process_documents_batch(documents, batch_size=10):
    """æ‰¹é‡å¤„ç†æ–‡æ¡£"""
    
    results = []
    
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i + batch_size]
        
        # å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
        batch_tasks = [
            process_single_document(doc) 
            for doc in batch
        ]
        
        batch_results = await asyncio.gather(*batch_tasks)
        results.extend(batch_results)
    
    return results
```

### 2. é”™è¯¯å¤„ç†ä¸é‡è¯•æœºåˆ¶

#### æ™ºèƒ½é‡è¯•
```python
import time
from functools import wraps

def retry_with_backoff(max_retries=3, base_delay=1):
    """æŒ‡æ•°é€€é¿é‡è¯•è£…é¥°å™¨"""
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise e
                    
                    delay = base_delay * (2 ** attempt)
                    print(f"é‡è¯• {attempt + 1}/{max_retries}ï¼Œç­‰å¾… {delay}ç§’...")
                    time.sleep(delay)
            
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@retry_with_backoff(max_retries=3, base_delay=2)
def call_api_with_retry():
    """å¸¦é‡è¯•çš„APIè°ƒç”¨"""
    return chat_model.invoke(messages)
```

#### ä¼˜é›…é™çº§
```python
def robust_document_retrieval(query, fallback_enabled=True):
    """å¥å£®çš„æ–‡æ¡£æ£€ç´¢"""
    
    try:
        # ä¸»è¦æ£€ç´¢æ–¹æ³•
        results = vector_search(query)
        
        if len(results) == 0 and fallback_enabled:
            # é™çº§åˆ°å…³é”®è¯æœç´¢
            results = keyword_search(query)
        
        return results
        
    except Exception as e:
        if fallback_enabled:
            # æœ€ç»ˆé™çº§ï¼šè¿”å›é»˜è®¤å›ç­”
            return get_default_response(query)
        else:
            raise e
```

### 3. ç›‘æ§ä¸æ—¥å¿—

#### æ€§èƒ½ç›‘æ§
```python
import time
from contextlib import contextmanager

@contextmanager
def performance_monitor(operation_name):
    """æ€§èƒ½ç›‘æ§ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    
    start_time = time.time()
    
    try:
        yield
    finally:
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"{operation_name} è€—æ—¶: {duration:.2f}ç§’")
        
        # è®°å½•åˆ°ç›‘æ§ç³»ç»Ÿ
        log_performance_metric(operation_name, duration)

# ä½¿ç”¨ç¤ºä¾‹
with performance_monitor("æ–‡æ¡£æ£€ç´¢"):
    results = vector_search(query)
```

#### ç»“æ„åŒ–æ—¥å¿—
```python
import logging
import json
from datetime import datetime

class StructuredLogger:
    """ç»“æ„åŒ–æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, name):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.INFO)
    
    def log_agent_action(self, action, query, result, duration):
        """è®°å½•Agentè¡Œä¸º"""
        
        log_data = {
            "timestamp": datetime.now().isoformat(),
            "action": action,
            "query": query,
            "result_length": len(str(result)),
            "duration": duration,
            "success": result is not None
        }
        
        self.logger.info(json.dumps(log_data, ensure_ascii=False))
```

### 4. è‡ªå®šä¹‰å·¥å…·å¼€å‘

#### å·¥å…·æ¥å£å®šä¹‰
```python
from langchain.tools import BaseTool
from typing import Optional, Type
from pydantic import BaseModel, Field

class SearchInput(BaseModel):
    """æœç´¢å·¥å…·è¾“å…¥æ¨¡å‹"""
    query: str = Field(description="æœç´¢æŸ¥è¯¢")
    max_results: int = Field(default=5, description="æœ€å¤§ç»“æœæ•°")

class CustomSearchTool(BaseTool):
    """è‡ªå®šä¹‰æœç´¢å·¥å…·"""
    
    name = "custom_search"
    description = "ç”¨äºæœç´¢ç›¸å…³ä¿¡æ¯çš„å·¥å…·"
    args_schema: Type[BaseModel] = SearchInput
    
    def _run(self, query: str, max_results: int = 5) -> str:
        """æ‰§è¡Œæœç´¢"""
        # å®ç°æœç´¢é€»è¾‘
        results = perform_search(query, max_results)
        return format_search_results(results)
    
    async def _arun(self, query: str, max_results: int = 5) -> str:
        """å¼‚æ­¥æ‰§è¡Œæœç´¢"""
        results = await perform_search_async(query, max_results)
        return format_search_results(results)
```

#### å·¥å…·ç»„åˆä½¿ç”¨
```python
def create_agent_with_tools():
    """åˆ›å»ºå¸¦æœ‰å¤šä¸ªå·¥å…·çš„Agent"""
    
    tools = [
        CustomSearchTool(),
        CalculatorTool(),
        WeatherTool(),
        DatabaseQueryTool()
    ]
    
    # åˆ›å»ºå·¥å…·æ‰§è¡ŒèŠ‚ç‚¹
    tool_node = ToolNode(tools)
    
    # æ„å»ºå·¥ä½œæµ
    workflow = StateGraph(AgentState)
    workflow.add_node("agent", agent_with_tools)
    workflow.add_node("tools", tool_node)
    
    # æ·»åŠ æ¡ä»¶è¾¹
    workflow.add_conditional_edges(
        "agent",
        tools_condition,
        {
            "tools": "tools",
            END: END
        }
    )
    
    return workflow.compile()
```

---

## å¸¸è§é—®é¢˜è§£ç­”

### Q1: å¦‚ä½•æé«˜æ£€ç´¢å‡†ç¡®æ€§ï¼Ÿ

#### A1: å¤šç»´åº¦ä¼˜åŒ–ç­–ç•¥

1. **æŸ¥è¯¢ä¼˜åŒ–**
```python
def optimize_query(original_query):
    """æŸ¥è¯¢ä¼˜åŒ–"""
    
    # 1. å…³é”®è¯æå–
    keywords = extract_keywords(original_query)
    
    # 2. åŒä¹‰è¯æ‰©å±•
    expanded_keywords = expand_synonyms(keywords)
    
    # 3. é‡æ„æŸ¥è¯¢
    optimized_query = reconstruct_query(expanded_keywords)
    
    return optimized_query
```

2. **æ–‡æ¡£é¢„å¤„ç†**
```python
def preprocess_documents(documents):
    """æ–‡æ¡£é¢„å¤„ç†"""
    
    processed_docs = []
    
    for doc in documents:
        # æ¸…ç†æ–‡æœ¬
        cleaned_text = clean_text(doc.page_content)
        
        # æå–å…³é”®ä¿¡æ¯
        key_info = extract_key_information(cleaned_text)
        
        # æ·»åŠ å…ƒæ•°æ®
        doc.metadata.update(key_info)
        
        processed_docs.append(doc)
    
    return processed_docs
```

3. **å¤šè·¯å¬å›**
```python
def multi_recall_search(query, top_k=10):
    """å¤šè·¯å¬å›æœç´¢"""
    
    # 1. å‘é‡æ£€ç´¢
    vector_results = vector_search(query, top_k)
    
    # 2. å…³é”®è¯æ£€ç´¢
    keyword_results = keyword_search(query, top_k)
    
    # 3. æ··åˆæ’åº
    combined_results = combine_and_rank(
        vector_results, 
        keyword_results
    )
    
    return combined_results[:top_k]
```

### Q2: å¦‚ä½•å¤„ç†é•¿æ–‡æ¡£ï¼Ÿ

#### A2: åˆ†å±‚å¤„ç†ç­–ç•¥

1. **å±‚æ¬¡åŒ–åˆ†å‰²**
```python
def hierarchical_split(document, max_chunk_size=1000):
    """å±‚æ¬¡åŒ–æ–‡æ¡£åˆ†å‰²"""
    
    # 1. æŒ‰ç« èŠ‚åˆ†å‰²
    sections = split_by_sections(document)
    
    chunks = []
    for section in sections:
        if len(section) <= max_chunk_size:
            chunks.append(section)
        else:
            # 2. æŒ‰æ®µè½åˆ†å‰²
            paragraphs = split_by_paragraphs(section)
            
            for paragraph in paragraphs:
                if len(paragraph) <= max_chunk_size:
                    chunks.append(paragraph)
                else:
                    # 3. æŒ‰å¥å­åˆ†å‰²
                    sentences = split_by_sentences(paragraph)
                    chunks.extend(sentences)
    
    return chunks
```

2. **æ‘˜è¦ç”Ÿæˆ**
```python
def generate_document_summary(document):
    """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦"""
    
    # 1. æå–å…³é”®æ®µè½
    key_paragraphs = extract_key_paragraphs(document)
    
    # 2. ç”Ÿæˆæ‘˜è¦
    summary = summarize_text(key_paragraphs)
    
    # 3. åˆ›å»ºæ‘˜è¦æ–‡æ¡£
    summary_doc = Document(
        page_content=summary,
        metadata={
            **document.metadata,
            "is_summary": True,
            "original_length": len(document.page_content)
        }
    )
    
    return summary_doc
```

### Q3: å¦‚ä½•ä¼˜åŒ–å“åº”é€Ÿåº¦ï¼Ÿ

#### A3: æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ

1. **å¼‚æ­¥å¤„ç†**
```python
async def async_rag_pipeline(query):
    """å¼‚æ­¥RAGæµç¨‹"""
    
    # å¹¶è¡Œæ‰§è¡Œå¤šä¸ªä»»åŠ¡
    tasks = [
        embed_query_async(query),
        preprocess_query_async(query),
        load_context_async()
    ]
    
    query_vector, processed_query, context = await asyncio.gather(*tasks)
    
    # å¼‚æ­¥æ£€ç´¢
    results = await search_async(query_vector)
    
    # å¼‚æ­¥ç”Ÿæˆ
    answer = await generate_async(processed_query, results)
    
    return answer
```

2. **é¢„è®¡ç®—ä¼˜åŒ–**
```python
class PrecomputedIndex:
    """é¢„è®¡ç®—ç´¢å¼•"""
    
    def __init__(self):
        self.frequent_queries = {}
        self.query_embeddings = {}
    
    def precompute_frequent_queries(self, queries):
        """é¢„è®¡ç®—å¸¸è§æŸ¥è¯¢"""
        
        for query in queries:
            # é¢„è®¡ç®—åµŒå…¥å‘é‡
            embedding = embedding_model.embed_query(query)
            self.query_embeddings[query] = embedding
            
            # é¢„è®¡ç®—æ£€ç´¢ç»“æœ
            results = vector_search_with_embedding(embedding)
            self.frequent_queries[query] = results
    
    def fast_search(self, query):
        """å¿«é€Ÿæœç´¢"""
        
        if query in self.frequent_queries:
            return self.frequent_queries[query]
        
        # æŸ¥æ‰¾ç›¸ä¼¼æŸ¥è¯¢
        similar_query = find_similar_query(query, self.query_embeddings)
        
        if similar_query:
            return self.frequent_queries[similar_query]
        
        # å¸¸è§„æœç´¢
        return regular_search(query)
```

### Q4: å¦‚ä½•å¤„ç†å¤šè¯­è¨€å†…å®¹ï¼Ÿ

#### A4: å¤šè¯­è¨€æ”¯æŒæ–¹æ¡ˆ

1. **è¯­è¨€æ£€æµ‹**
```python
from langdetect import detect

def detect_and_process_language(text):
    """æ£€æµ‹å¹¶å¤„ç†è¯­è¨€"""
    
    try:
        language = detect(text)
        
        if language == 'zh':
            return process_chinese_text(text)
        elif language == 'en':
            return process_english_text(text)
        else:
            return process_other_language(text, language)
            
    except Exception:
        # é»˜è®¤å¤„ç†
        return process_default(text)
```

2. **å¤šè¯­è¨€åµŒå…¥**
```python
class MultilingualEmbedding:
    """å¤šè¯­è¨€åµŒå…¥æ¨¡å‹"""
    
    def __init__(self):
        self.models = {
            'zh': GoogleGenerativeAIEmbeddings(model="embedding-001"),
            'en': GoogleGenerativeAIEmbeddings(model="embedding-001"),
            'multilingual': GoogleGenerativeAIEmbeddings(model="embedding-multilingual")
        }
    
    def embed_text(self, text, language=None):
        """å¤šè¯­è¨€æ–‡æœ¬åµŒå…¥"""
        
        if language is None:
            language = detect(text)
        
        if language in self.models:
            return self.models[language].embed_query(text)
        else:
            return self.models['multilingual'].embed_query(text)
```

### Q5: å¦‚ä½•è¯„ä¼°ç³»ç»Ÿæ€§èƒ½ï¼Ÿ

#### A5: è¯„ä¼°æŒ‡æ ‡ä½“ç³»

1. **æ£€ç´¢è´¨é‡è¯„ä¼°**
```python
def evaluate_retrieval_quality(test_cases):
    """è¯„ä¼°æ£€ç´¢è´¨é‡"""
    
    metrics = {
        'precision': [],
        'recall': [],
        'f1_score': [],
        'mrr': []  # Mean Reciprocal Rank
    }
    
    for case in test_cases:
        query = case['query']
        expected_docs = case['relevant_docs']
        
        # æ‰§è¡Œæ£€ç´¢
        retrieved_docs = vector_search(query, top_k=10)
        
        # è®¡ç®—æŒ‡æ ‡
        precision = calculate_precision(retrieved_docs, expected_docs)
        recall = calculate_recall(retrieved_docs, expected_docs)
        f1 = calculate_f1_score(precision, recall)
        mrr = calculate_mrr(retrieved_docs, expected_docs)
        
        metrics['precision'].append(precision)
        metrics['recall'].append(recall)
        metrics['f1_score'].append(f1)
        metrics['mrr'].append(mrr)
    
    # è®¡ç®—å¹³å‡å€¼
    avg_metrics = {
        key: sum(values) / len(values)
        for key, values in metrics.items()
    }
    
    return avg_metrics
```

2. **ç”Ÿæˆè´¨é‡è¯„ä¼°**
```python
def evaluate_generation_quality(test_cases):
    """è¯„ä¼°ç”Ÿæˆè´¨é‡"""
    
    from rouge import Rouge
    from bert_score import score
    
    rouge = Rouge()
    
    rouge_scores = []
    bert_scores = []
    
    for case in test_cases:
        query = case['query']
        reference_answer = case['reference_answer']
        
        # ç”Ÿæˆç­”æ¡ˆ
        generated_answer = rag_pipeline(query)
        
        # ROUGEè¯„åˆ†
        rouge_score = rouge.get_scores(generated_answer, reference_answer)
        rouge_scores.append(rouge_score)
        
        # BERTè¯„åˆ†
        P, R, F1 = score([generated_answer], [reference_answer], lang='zh')
        bert_scores.append(F1.item())
    
    return {
        'rouge_scores': rouge_scores,
        'bert_scores': bert_scores,
        'avg_bert_score': sum(bert_scores) / len(bert_scores)
    }
```

---

## æ€»ç»“

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»äº†LangChain Agentçš„æ ¸å¿ƒæ¦‚å¿µã€å·¥ä½œåŸç†å’Œå®è·µæŠ€å·§ã€‚é€šè¿‡å­¦ä¹ æœ¬æŒ‡å—ï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

1. **ç†è§£Agentçš„æœ¬è´¨**ï¼šæŒæ¡æ™ºèƒ½ä½“çš„åŸºæœ¬æ¦‚å¿µå’Œå·¥ä½œæœºåˆ¶
2. **æŒæ¡LangGraph**ï¼šå­¦ä¼šä½¿ç”¨çŠ¶æ€å›¾ç®¡ç†å¤æ‚å·¥ä½œæµ
3. **æ„å»ºRAGç³»ç»Ÿ**ï¼šå®ç°å®Œæ•´çš„æ£€ç´¢å¢å¼ºç”Ÿæˆåº”ç”¨
4. **ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½**ï¼šåº”ç”¨å„ç§ä¼˜åŒ–æŠ€å·§æå‡ç³»ç»Ÿæ•ˆæœ
5. **è§£å†³å®é™…é—®é¢˜**ï¼šå¤„ç†å¼€å‘è¿‡ç¨‹ä¸­é‡åˆ°çš„å¸¸è§é—®é¢˜

### å­¦ä¹ å»ºè®®

1. **å¾ªåºæ¸è¿›**ï¼šä»ç®€å•çš„Agentå¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚æ€§
2. **åŠ¨æ‰‹å®è·µ**ï¼šé€šè¿‡å®é™…é¡¹ç›®åŠ æ·±ç†è§£
3. **æŒç»­å­¦ä¹ **ï¼šå…³æ³¨LangChainå’ŒLangGraphçš„æœ€æ–°å‘å±•
4. **ç¤¾åŒºå‚ä¸**ï¼šå‚ä¸å¼€æºç¤¾åŒºï¼Œåˆ†äº«ç»éªŒå’Œå­¦ä¹ 

### è¿›ä¸€æ­¥å­¦ä¹ èµ„æº

- [LangChainå®˜æ–¹æ–‡æ¡£](https://python.langchain.com/)
- [LangGraphæ•™ç¨‹](https://langchain-ai.github.io/langgraph/)
- [Google Gemini APIæ–‡æ¡£](https://ai.google.dev/docs)
- [Qdrantå‘é‡æ•°æ®åº“æ–‡æ¡£](https://qdrant.tech/documentation/)

å¸Œæœ›è¿™ä¸ªæŒ‡å—èƒ½å¤Ÿå¸®åŠ©ä½ åœ¨LangChain Agentçš„å­¦ä¹ å’Œå¼€å‘é“è·¯ä¸Šå–å¾—æˆåŠŸï¼